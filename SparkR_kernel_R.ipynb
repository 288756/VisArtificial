{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/288756/VisArtificial/blob/master/SparkR_kernel_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANTE: este notebook se tiene que crear utilizando el kernel de R. Para ello, se puede utilizar el siguiente enlace: https://colab.research.google.com/notebook#create=true&language=r"
      ],
      "metadata": {
        "id": "FXGpuwe6PiiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prueba de kernel R"
      ],
      "metadata": {
        "id": "l9QaWKgqPliX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "e3J7Jf14Pdia",
        "outputId": "cd3ce297-a449-4ce9-91e0-2ebe244f4ec3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'kernel'</li><li>'R'</li></ol>\n"
            ],
            "text/markdown": "1. 'kernel'\n2. 'R'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 'kernel'\n\\item 'R'\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] \"kernel\" \"R\"     "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"IMPORTANTE: arriba tiene que salir Kernel R. Si no es así, se tiene que cambiar al Kernel de R\"\n"
          ]
        }
      ],
      "source": [
        "c(\"kernel\", \"R\")\n",
        "print(\"IMPORTANTE: arriba tiene que salir Kernel R. Si no es así, se tiene que cambiar al Kernel de R\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación"
      ],
      "metadata": {
        "id": "iufX4uV5QTdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación Java"
      ],
      "metadata": {
        "id": "13a3dhEyQWfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system(\"apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\", intern = FALSE)"
      ],
      "metadata": {
        "id": "hHl6qe5QPhlK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "java_home <- system(\"readlink -f /usr/bin/java | sed 's:/bin/java::'\", intern = TRUE)\n",
        "Sys.setenv(\"JAVA_HOME\" = java_home)"
      ],
      "metadata": {
        "id": "o8RfFHrfQAVm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paste(\"Java está en el path:\", Sys.getenv(\"JAVA_HOME\"), \"(IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63gPqk9AQr3W",
        "outputId": "c6620bab-1f1b-432c-d235-5d944cb80955"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Java está en el path: /usr/lib/jvm/java-11-openjdk-amd64 (IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtener versión de Spark"
      ],
      "metadata": {
        "id": "vYPB_JKURcZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_version <- system(\"curl -s https://archive.apache.org/dist/spark/ | grep -oP 'spark-*.*/\\\"' | grep --invert-match -e '-.*-' | awk -F '/\\\"' '{print $1}' | cut -d '-' -f2 | tail -n 1\", intern = TRUE)\n",
        "Sys.setenv(\"SPARK_VERSION\" = spark_version)"
      ],
      "metadata": {
        "id": "qRh0r8MrRJI4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paste(\"IMPORTANTE: La versión estable de Spark es:\", Sys.getenv(\"SPARK_VERSION\"), \"(si no sale nada, hay que ponerla manualmente)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ga3AO_XRjIE",
        "outputId": "bd1fa55a-f089-4db4-fa44-d1983390e5a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"IMPORTANTE: La versión estable de Spark es: 3.5.1 (si no sale nada, hay que ponerla manualmente)\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descargar Spark"
      ],
      "metadata": {
        "id": "uKQ0_18rSr6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_download_url <- paste0(\"https://dlcdn.apache.org/spark/spark-\", Sys.getenv(\"SPARK_VERSION\"), \"/spark-\", Sys.getenv(\"SPARK_VERSION\"), \"-bin-hadoop3.tgz\")\n",
        "#spark_download_url <- paste0(\"https://archive.apache.org/dist/spark/spark-\", Sys.getenv(\"SPARK_VERSION\"), \"/spark-\", Sys.getenv(\"SPARK_VERSION\"), \"-bin-hadoop3.tgz\")\n",
        "system(paste(\"wget\", spark_download_url), intern = TRUE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rJOXuE-uS04O",
        "outputId": "550688e2-ace9-481d-a2e2-92afc3977538"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [],
            "text/markdown": "",
            "text/latex": "",
            "text/plain": [
              "character(0)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_folder_name <- paste0(\"spark-\", Sys.getenv(\"SPARK_VERSION\"), \"-bin-hadoop3\")\n",
        "spark_tgz_name <- paste0(spark_folder_name,\".tgz\")\n",
        "system(paste(\"ls\", spark_tgz_name), intern = TRUE)\n",
        "print(paste(\"IMPORTANTE: en la anterior línea debería salir\", spark_tgz_name, \". Si no es así, se tendría que descargar manualmente\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3k9ocMyxT_sO",
        "outputId": "3488d6bb-51bb-43c1-c392-f9fe6853e98f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "'spark-3.5.1-bin-hadoop3.tgz'"
            ],
            "text/markdown": "'spark-3.5.1-bin-hadoop3.tgz'",
            "text/latex": "'spark-3.5.1-bin-hadoop3.tgz'",
            "text/plain": [
              "[1] \"spark-3.5.1-bin-hadoop3.tgz\""
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"IMPORTANTE: en la anterior línea debería salir spark-3.5.1-bin-hadoop3.tgz . Si no es así, se tendría que descargar manualmente\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descomprimir Spark"
      ],
      "metadata": {
        "id": "i_yjZ8TQU_IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system(paste(\"tar xf\", spark_tgz_name))"
      ],
      "metadata": {
        "id": "QXHc86cYVBMm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system(paste(\"ls -d\",  spark_folder_name), intern = TRUE)\n",
        "print(paste(\"IMPORTANTE: en la anterior línea debería salir\", spark_folder_name, \". Si no es así, se tendría que descomprimir manualmente\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hJXeVIxdVP4h",
        "outputId": "875b7554-201e-4080-ef16-c7fc01dad87c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "'spark-3.5.1-bin-hadoop3'"
            ],
            "text/markdown": "'spark-3.5.1-bin-hadoop3'",
            "text/latex": "'spark-3.5.1-bin-hadoop3'",
            "text/plain": [
              "[1] \"spark-3.5.1-bin-hadoop3\""
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"IMPORTANTE: en la anterior línea debería salir spark-3.5.1-bin-hadoop3 . Si no es así, se tendría que descomprimir manualmente\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mover la carpeta de Spark a /usr/local"
      ],
      "metadata": {
        "id": "WCzJs9PDV8uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system(paste(\"mv\", spark_folder_name, \"/usr/local/\"))"
      ],
      "metadata": {
        "id": "86p89rrkV_xY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_path = paste0(\"/usr/local/\", spark_folder_name)\n",
        "\n",
        "system(paste(\"ls -d\", spark_path), intern = TRUE)\n",
        "print(paste(\"IMPORTANTE: en la anterior línea debería salir la carpeta\", spark_path, \". Si no es así, habría que moverla manualmente a esa ubicación\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "l34ZlvbaWXe-",
        "outputId": "2179775f-e8ba-4255-d419-b1a0f977444c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "'/usr/local/spark-3.5.1-bin-hadoop3'"
            ],
            "text/markdown": "'/usr/local/spark-3.5.1-bin-hadoop3'",
            "text/latex": "'/usr/local/spark-3.5.1-bin-hadoop3'",
            "text/plain": [
              "[1] \"/usr/local/spark-3.5.1-bin-hadoop3\""
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"IMPORTANTE: en la anterior línea debería salir la carpeta /usr/local/spark-3.5.1-bin-hadoop3 . Si no es así, habría que moverla manualmente a esa ubicación\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear variable de entorno de SPARK_HOME"
      ],
      "metadata": {
        "id": "djv_6xxBW0X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sys.setenv(\"SPARK_HOME\" =  spark_path)\n",
        "Sys.setenv(\"PATH\" = paste(Sys.getenv(\"PATH\"), paste(Sys.getenv(\"SPARK_HOME\"), \"bin\", sep = \"/\"), sep = \":\"))"
      ],
      "metadata": {
        "id": "-0ULfJjbW35J"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paste(\"Spark está en el path:\", Sys.getenv(\"SPARK_HOME\"), \"(IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alz-WdeeW-nU",
        "outputId": "cc82f2d0-1bdb-4dad-d659-de050a24dcda"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Spark está en el path: /usr/local/spark-3.5.1-bin-hadoop3 (IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobar instalación"
      ],
      "metadata": {
        "id": "tu0s0tm9MIOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(processx)\n",
        "#IMPORTANTE: si la librería no se carga correctamente, se tiene que instalar manualmente\""
      ],
      "metadata": {
        "id": "UXkflgloOQ_v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_version_outcome <- run(command = \"spark-submit\", args = \"--version\", echo  = TRUE)\n",
        "print(\"IMPORTANTE: en las anteriores líneas debería mostrarnos la versión de Spark. Si no es así, habría que establecer manualmente la variable PATH añadiéndole la carpeta bin de Spark\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmVFYjWaNr8o",
        "outputId": "371ffe05-5529-4040-cbc6-d502800b6fbe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWelcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \u001b[39m\u001b[31m\n",
            "\u001b[39m\u001b[31mUsing Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.22\n",
            "Branch HEAD\u001b[39m\u001b[31m\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\u001b[39m\u001b[31m\n",
            "\u001b[39m\u001b[31mUrl https://github.com/apache/spark\n",
            "Type --help for more information.\n",
            "\u001b[39m[1] \"IMPORTANTE: en las anteriores líneas debería mostrarnos la versión de Spark. Si no es así, habría que establecer manualmente la variable PATH añadiéndole la carpeta bin de Spark\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arrancar SparkR"
      ],
      "metadata": {
        "id": "_cH6fkw_Sg28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n",
        "sparkR.session(master = \"local[*]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "aSEYWtqhSga5",
        "outputId": "70ce216f-52d1-490e-ccad-913fa4553812"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Attaching package: ‘SparkR’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    cov, filter, lag, na.omit, predict, sd, var, window\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:base’:\n",
            "\n",
            "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
            "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
            "\n",
            "\n",
            "Spark package found in SPARK_HOME: /usr/local/spark-3.5.1-bin-hadoop3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching java with spark-submit command /usr/local/spark-3.5.1-bin-hadoop3/bin/spark-submit   sparkr-shell /tmp/RtmpI9U3d1/backend_port1f8304057f4 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Java ref type org.apache.spark.sql.SparkSession id 1 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecutar el código interactivo"
      ],
      "metadata": {
        "id": "Ab2oMcQlYgza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head(as.DataFrame(data.frame(1,2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "OxH-sxGAR631",
        "outputId": "ca9ae5ad-bf0a-47d4-984f-0d8e0a4617ba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 1 × 2</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>1</td><td>2</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 1 × 2\n\n| <!--/--> | X1 &lt;dbl&gt; | X2 &lt;dbl&gt; |\n|---|---|---|\n| 1 | 1 | 2 |\n\n",
            "text/latex": "A data.frame: 1 × 2\n\\begin{tabular}{r|ll}\n  & X1 & X2\\\\\n  & <dbl> & <dbl>\\\\\n\\hline\n\t1 & 1 & 2\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  X1 X2\n",
              "1 1  2 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecutar programa"
      ],
      "metadata": {
        "id": "sIbWiHywY7EH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear archivo tempMax.R"
      ],
      "metadata": {
        "id": "MJDmJUkKQwov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creamos el archivo ciudadMax.R con el script corregido\n",
        "system(r'(cat << EOF > ciudadMax.R\n",
        "#!/usr/bin/env -S spark-submit\n",
        "\n",
        "# Indicamos la libreria SparkR\n",
        "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n",
        "\n",
        "# Creamos la sesion Spark\n",
        "sparkR.session(appName = \"ciudadMayorVentaLicor\")\n",
        "# Definimos el esquema de los datos\n",
        "\n",
        "miSchema <- structType(\n",
        "  structField(\"Invoice/Item Number\", \"string\"),\n",
        "  structField(\"Date\", \"string\"),\n",
        "  structField(\"Store Number\", \"string\"),\n",
        "  structField(\"Store Name\", \"string\"),\n",
        "  structField(\"Address\", \"string\"),\n",
        "  structField(\"City\", \"string\"),\n",
        "  structField(\"Zip Code\", \"string\"),\n",
        "  structField(\"Store Location\", \"string\"),\n",
        "  structField(\"County Number\", \"string\"),\n",
        "  structField(\"County\", \"string\"),\n",
        "  structField(\"Category\", \"string\"),\n",
        "  structField(\"Category Name\", \"string\"),\n",
        "  structField(\"Vendor Number\", \"string\"),\n",
        "  structField(\"Vendor Name\", \"string\"),\n",
        "  structField(\"Item Number\", \"string\"),\n",
        "  structField(\"Item Description\", \"string\"),\n",
        "  structField(\"Pack\", \"string\"),\n",
        "  structField(\"Bottle Volume (ml)\", \"string\"),\n",
        "  structField(\"State Bottle Cost\", \"string\"),\n",
        "  structField(\"State Bottle Retail\", \"string\"),\n",
        "  structField(\"Bottles Sold\", \"string\"),\n",
        "  structField(\"Sale (Dollars)\", \"string\"),\n",
        "  structField(\"Volume Sold (Liters)\", \"string\"),\n",
        "  structField(\"Volume Sold (Gallons)\", \"string\")\n",
        ")\n",
        "\n",
        "# Obtenemos la ubicacion de los datos de entrada y salida\n",
        "args <- commandArgs(trailingOnly = TRUE)\n",
        "if (length(args) != 2) {\n",
        "  stop(\"Se tienen que insertar 2 argumentos\", call. = FALSE)\n",
        "}\n",
        "path_entrada <- args[1]\n",
        "path_salida <- args[2]\n",
        "\n",
        "# Leemos los datos\n",
        "datos <- read.df(path = path_entrada, header = TRUE, source = \"csv\", delimiter = \",\", schema = miSchema)\n",
        "\n",
        "# Obtenemos la suma de dólares de todas las ventas de licor por ciudad\n",
        "ventas_por_ciudad <- agg(groupBy(datos, datos[[\"City\"]]), \"Total_Venta\" = sum(datos[[\"Sale (Dollars)\"]]))\n",
        "\n",
        "# Ordenamos las ciudades de mayor a menor\n",
        "ciudades <- orderBy(ventas_por_ciudad, desc(ventas_por_ciudad$\"Total_Venta\"))\n",
        "\n",
        "# Tomamos la ciudad con la mayor suma total de ventas de licor\n",
        "ciudad_mayor_venta <- head(ciudades, n = 1)\n",
        "\n",
        "# Guardamos el resultado en el archivo de salida\n",
        "write.table(ciudad_mayor_venta[1], file = path_salida, row.names = FALSE, col.names = FALSE, quote = FALSE)\n",
        "\n",
        "EOF\n",
        ")')\n"
      ],
      "metadata": {
        "id": "U3MWlu-nklTr"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar que el EOF marca el final del archivo (del programa)"
      ],
      "metadata": {
        "id": "YcgO_jiuR2cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANTE fijarse en el uso de ' y \". Por ejemplo en el caso de temperatura$anyo, mejor ponerlo como temperatura$\"anyo\" para evitar problemas"
      ],
      "metadata": {
        "id": "dHHZyruYi9wX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear datos"
      ],
      "metadata": {
        "id": "HTwANWs4Q0mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar que el EOF marca el final del archivo"
      ],
      "metadata": {
        "id": "LmYFO4eqR7R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar programa"
      ],
      "metadata": {
        "id": "ZwNFOsXmRw7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_submit_outcome <- run(command = \"spark-submit\", args = c(\"ciudadMax.R\", \"licores.csv\", \"salida\"), echo  = TRUE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmjD1DLWdxGs",
        "outputId": "8f9f384d-fbad-4238-fce4-b48ff22130d8"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attaching package: ‘SparkR’\n",
            "\n",
            "The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    cov, filter, lag, na.omit, predict, sd, var, window\n",
            "\n",
            "The following objects are masked from ‘package:base’:\n",
            "\n",
            "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
            "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
            "\n",
            "Spark package found in SPARK_HOME: /usr/local/spark-3.5.1-bin-hadoop3\n",
            "\u001b[31m24/05/06 17:12:05 INFO SparkContext: Running Spark version 3.5.1\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:05 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:05 INFO SparkContext: Java version 11.0.22\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO ResourceUtils: ==============================================================\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/05/06 17:12:06 INFO ResourceUtils: ==============================================================\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SparkContext: Submitted application: ciudadMayorVentaLicor\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO ResourceProfile: Limiting resource is cpu\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SecurityManager: Changing view acls to: root\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SecurityManager: Changing modify acls to: root\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SecurityManager: Changing view acls groups to: \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SecurityManager: Changing modify acls groups to: \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO Utils: Successfully started service 'sparkDriver' on port 43575.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SparkEnv: Registering MapOutputTracker\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SparkEnv: Registering BlockManagerMaster\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e9e2211f-977a-4967-97e8-37ba601aef5d\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:06 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO SparkContext: Added file file:/content/ciudadMax.R at file:/content/ciudadMax.R with timestamp 1715015525745\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Utils: Copying /content/ciudadMax.R to /tmp/spark-fe7025ed-26a7-45fd-a893-de532cbbc8dc/userFiles-36fe0f06-c4de-402f-8a51-265e59157aa9/ciudadMax.R\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Executor: Starting executor ID driver on host 882b658e5d15\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Executor: Java version 11.0.22\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7b0c0dbe for default.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Executor: Fetching file:/content/ciudadMax.R with timestamp 1715015525745\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Utils: /content/ciudadMax.R has been previously copied to /tmp/spark-fe7025ed-26a7-45fd-a893-de532cbbc8dc/userFiles-36fe0f06-c4de-402f-8a51-265e59157aa9/ciudadMax.R\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41533.\n",
            "24/05/06 17:12:07 INFO NettyBlockTransferService: Server created on 882b658e5d15:41533\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 882b658e5d15, 41533, None)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO BlockManagerMasterEndpoint: Registering block manager 882b658e5d15:41533 with 434.4 MiB RAM, BlockManagerId(driver, 882b658e5d15, 41533, None)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 882b658e5d15, 41533, None)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 882b658e5d15, 41533, None)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:08 INFO HiveConf: Found configuration file null\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:08 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "\u001b[39mJava ref type org.apache.spark.sql.SparkSession id 1 \n",
            "\u001b[31m24/05/06 17:12:09 INFO InMemoryFileIndex: It took 87 ms to list leaf files for 1 paths.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:12 INFO FileSourceStrategy: Pushed Filters: \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:13 INFO CodeGenerator: Code generated in 867.973373 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.6 KiB, free 434.2 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 434.2 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 882b658e5d15:41533 (size: 34.7 KiB, free: 434.4 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:14 INFO SparkContext: Created broadcast 0 from dfToCols at NativeMethodAccessorImpl.java:0\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5347235 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO DAGScheduler: Registering RDD 3 (dfToCols at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO DAGScheduler: Got map stage job 0 (dfToCols at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (dfToCols at NativeMethodAccessorImpl.java:0)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO DAGScheduler: Missing parents: List()\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at dfToCols at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 43.4 KiB, free 434.1 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 434.1 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 882b658e5d15:41533 (size: 20.1 KiB, free: 434.3 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at dfToCols at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (882b658e5d15, executor driver, partition 0, PROCESS_LOCAL, 8346 bytes) \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (882b658e5d15, executor driver, partition 1, PROCESS_LOCAL, 8346 bytes) \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO CodeGenerator: Code generated in 80.697123 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:15 INFO CodeGenerator: Code generated in 25.472002 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:16 INFO CodeGenerator: Code generated in 29.033774 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:16 INFO CodeGenerator: Code generated in 11.680619 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:16 INFO CodeGenerator: Code generated in 15.377273 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:16 INFO FileScanRDD: Reading File path: file:///content/licores.csv, range: 0-5347235, partition values: [empty row]\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:16 INFO FileScanRDD: Reading File path: file:///content/licores.csv, range: 5347235-6500167, partition values: [empty row]\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:16 INFO CodeGenerator: Code generated in 17.659746 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: \"\"Zip Code\"\", \"\"Volume Sold (Liters)\"\"\n",
            " Schema: City, Sale (Dollars)\n",
            "Expected: City but found: \"\"Zip Code\"\"\n",
            "CSV file: file:///content/licores.csv\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2765 bytes result sent to driver\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1647 ms on 882b658e5d15 (executor driver) (1/2)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2765 bytes result sent to driver\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1868 ms on 882b658e5d15 (executor driver) (2/2)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: ShuffleMapStage 0 (dfToCols at NativeMethodAccessorImpl.java:0) finished in 2.106 s\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: looking for newly runnable stages\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: running: Set()\n",
            "24/05/06 17:12:17 INFO DAGScheduler: waiting: Set()\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: failed: Set()\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO CodeGenerator: Code generated in 37.392107 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO CodeGenerator: Code generated in 34.988834 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO SparkContext: Starting job: dfToCols at NativeMethodAccessorImpl.java:0\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: Got job 1 (dfToCols at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: Final stage: ResultStage 2 (dfToCols at NativeMethodAccessorImpl.java:0)\n",
            "24/05/06 17:12:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: Missing parents: List()\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at dfToCols at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 46.8 KiB, free 434.1 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.8 KiB, free 434.0 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 882b658e5d15:41533 (size: 21.8 KiB, free: 434.3 MiB)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at dfToCols at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (882b658e5d15, executor driver, partition 0, NODE_LOCAL, 7767 bytes) \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO CodeGenerator: Code generated in 19.750737 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO ShuffleBlockFetcherIterator: Getting 2 (39.3 KiB) non-empty blocks including 2 (39.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:17 INFO CodeGenerator: Code generated in 41.363445 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 5169 bytes result sent to driver\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 319 ms on 882b658e5d15 (executor driver) (1/1)\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:18 INFO DAGScheduler: ResultStage 2 (dfToCols at NativeMethodAccessorImpl.java:0) finished in 0.360 s\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "\u001b[39m\u001b[31m24/05/06 17:12:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:18 INFO DAGScheduler: Job 1 finished: dfToCols at NativeMethodAccessorImpl.java:0, took 0.417459 s\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:19 INFO CodeGenerator: Code generated in 33.116313 ms\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/05/06 17:12:20 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO SparkUI: Stopped Spark web UI at http://882b658e5d15:4041\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO MemoryStore: MemoryStore cleared\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO BlockManager: BlockManager stopped\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO SparkContext: Successfully stopped SparkContext\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO ShutdownHookManager: Shutdown hook called\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-fbae9d57-ca06-45d2-bcf1-37ab916392f4\n",
            "\u001b[39m\u001b[31m24/05/06 17:12:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe7025ed-26a7-45fd-a893-de532cbbc8dc\n",
            "\u001b[39m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobar salida"
      ],
      "metadata": {
        "id": "sf4whLva4qV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos que se ha creado la carpeta ./miSalida con el resultado"
      ],
      "metadata": {
        "id": "kriuXS9x4zo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls_outcome = run(command = \"ls\", args = c(\"-l\", \"salida\"), echo = TRUE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo_uBc3F3UzC",
        "outputId": "5c3ed858-155f-40b4-b92e-b601f9326004"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 15 May  6 17:12 salida\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abrimos el archivo que contiene la salida"
      ],
      "metadata": {
        "id": "gA1iqmvh4yBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "writeLines(system(\"cat salida/*\", intern = TRUE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0exwYoue0X8",
        "outputId": "75e8e75f-06b9-4af5-c79d-a418933637cb"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in system(\"cat salida/*\", intern = TRUE):\n",
            "“running command 'cat salida/*' had status 1”\n"
          ]
        }
      ]
    }
  ]
}
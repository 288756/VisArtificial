{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPg29VBK4dl84/d5Z9xSIrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/288756/VisArtificial/blob/master/Miniproyecto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxNz0gayJjJ7",
        "outputId": "dfc27284-e8a3-4645-a50c-0f6dbb1c8436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar el Google Drive en el directorio del proyecto y descomprimir el fichero con los datos\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -n '/content/gdrive/My Drive/vision-artificial.zip' >> /dev/null  # ACTUALIZAR: ruta al fichero comprimido\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Especificar las rutas al directorio con las imágenes y al fichero con las etiquetas\n",
        "data_path = '/content/'\n",
        "imgtrain_dir = data_path + \"train/images/\"\n",
        "csvtrain_file = data_path + \"train.csv\"\n",
        "\n",
        "imgtest_dir = data_path + \"test/images/\"\n",
        "csvtest_file = data_path + \"test.csv\"\n",
        "# Leer el fichero CSV con las etiquetas\n",
        "\n",
        "dftrain = pd.read_csv(csvtrain_file, dtype = {\"class\": \"category\"})\n",
        "dftest = pd.read_csv(csvtest_file, dtype = {\"class\": \"category\"})\n",
        "# Codificar las etiquetas utilizando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "dftrain['class_encoded'] = label_encoder.fit_transform(dftrain['class'])\n",
        "\n",
        "# Convertir las etiquetas codificadas en un vector one-hot\n",
        "class_one_hot = pd.get_dummies(dftrain['class_encoded'], prefix='class')\n",
        "\n",
        "# Renombrar las columnas del vector one-hot\n",
        "class_one_hot.columns = ['normal', 'benign', 'malignant']\n",
        "\n",
        "# Concatenar el DataFrame original con las etiquetas one-hot\n",
        "dftrain = pd.concat([dftrain[['image_filename']], class_one_hot], axis=1)\n",
        "\n",
        "# Imprimir las primeras filas del DataFrame para verificar\n",
        "print(len(imgtrain_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5eppKQHK8GS",
        "outputId": "e09fd77d-3507-4c98-a422-19d78e1485c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = int(len(dftrain) * 0.2)\n",
        "dftrain = dftrain.sample(frac=1).reset_index(drop=True)  # barajar el dataframe\n",
        "dfval = dftrain[:val_size]\n",
        "print(f'Número de ejemplos del conjunto de entrenamiento: {dftrain.shape[0]}')\n",
        "print(f'Número de ejemplos del conjunto de validación: {dfval.shape[0]}')\n",
        "print(f'Número de ejemplos del conjunto de test: {dftest.shape[0]}')\n",
        "dftrain = dftrain.reset_index(drop=True)\n",
        "dfval = dfval.reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTDHF5_xMuCc",
        "outputId": "b4b917f3-4717-4e21-d468-6a497e342ef8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de ejemplos del conjunto de entrenamiento: 337\n",
            "Número de ejemplos del conjunto de validación: 67\n",
            "Número de ejemplos del conjunto de test: 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "archivos = os.listdir(imgtrain_dir)\n",
        "anchuras = []\n",
        "alturas = []\n",
        "for archivo in archivos:\n",
        "      imagen = Image.open(os.path.join(imgtrain_dir, archivo))\n",
        "      ancho, alto = imagen.size\n",
        "      anchuras.append(ancho)\n",
        "      alturas.append(alto)\n",
        "media_anchura = sum(anchuras) / len(anchuras)\n",
        "media_altura = sum(alturas) / len(alturas)\n",
        "\n",
        "print(\"Media de anchura:\", media_anchura)\n",
        "print(\"Media de altura:\", media_altura)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYr-HxGamubc",
        "outputId": "a4e8b3d0-a345-42fe-c496-3878ebc59dfc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Media de anchura: 634.9139465875371\n",
            "Media de altura: 514.9317507418398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "# Dimensiones deseadas de la imagen\n",
        "img_width, img_height = 634, 514\n",
        "n_channels = 3                # número de canales (RGB)\n",
        "n_classes = 3                 # número de clases\n",
        "x_col = 'image_filename'      # nombres de las columnas en el fichero CSV\n",
        "y_col = ['normal', 'benign', 'malignant']  # lista de nombres de las columnas de las etiquetas\n",
        "\n",
        "# Cargar y preprocesar imágenes\n",
        "def load_and_preprocess_image(image_filename, label_one_hot):\n",
        "    image_path = tf.strings.join([imgtrain_dir, image_filename])\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=n_channels)\n",
        "    # Redimensionar la imagen al tamaño deseado con relleno de ceros si es necesario\n",
        "    image = tf.image.resize_with_pad(image, img_width, img_height)\n",
        "    image = image / 255.0                               # normalización\n",
        "    label = label_one_hot                                # Utilizar etiquetas codificadas como vector one-hot\n",
        "    return image, label\n",
        "\n",
        "# Crear conjunto de datos\n",
        "def get_dataset(df):\n",
        "    image_filenames = df[x_col].values\n",
        "    labels = df[y_col].values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_filenames, labels))\n",
        "    dataset = dataset.map(load_and_preprocess_image)\n",
        "    return dataset\n",
        "\n",
        "# Crear los conjuntos de datos y preparar los lotes\n",
        "batch_size = 32\n",
        "train_dataset = get_dataset(dftrain).batch(batch_size)\n",
        "val_dataset = get_dataset(dfval).batch(batch_size)\n",
        "\n",
        "print(f'Número de lotes del conjunto de entrenamiento: {len(train_dataset)}')\n",
        "print(f'Número de lotes del conjunto de validación: {len(val_dataset)}')\n"
      ],
      "metadata": {
        "id": "orVxnj58NRj3",
        "outputId": "53034fa3-8bd9-499d-f502-472f4e064eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de lotes del conjunto de entrenamiento: 11\n",
            "Número de lotes del conjunto de validación: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def simple_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the input shape (width, height, channels)\n",
        "input_shape = (img_width, img_height, 3)\n",
        "# Define the number of classes\n",
        "num_classes = 3  # Assuming you have 3 classes\n",
        "\n",
        "# Create the model\n",
        "model = simple_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use this loss function for integer labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Ys_7nr_MF-Qf",
        "outputId": "9bf7190d-24d2-4e8c-b32a-46bde63f9c78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 632, 512, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 316, 256, 32)      0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 314, 254, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 157, 127, 64)      0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 155, 125, 64)      36928     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 1240000)           0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                79360064  \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79416579 (302.95 MB)\n",
            "Trainable params: 79416579 (302.95 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Entrenar el modelo con los datos preparados previamente\n",
        "history = model.fit(train_dataset,\n",
        "          epochs=20,   # número de epochs\n",
        "          verbose=2,  # muestra información al finalizar cada epoch\n",
        "          validation_data=val_dataset)\n",
        "\n",
        "# Imprimir el error mínimo de entrenamiento y validación\n",
        "train_trace = np.array(history.history['loss'])\n",
        "print(f'\\nError mínimo en entrenamiento: {min(train_trace):.6f}')\n",
        "\n",
        "val_trace = np.array(history.history['val_loss'])\n",
        "print(f'Error mínimo en validación: {min(val_trace):.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWaU8MKGg3P-",
        "outputId": "f53fe0c3-eb4e-48a5-da35-78d6d9d07f3c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 - 18s - loss: 8.9442 - accuracy: 0.4332 - val_loss: 0.9560 - val_accuracy: 0.5075 - 18s/epoch - 2s/step\n",
            "Epoch 2/20\n",
            "11/11 - 3s - loss: 0.9300 - accuracy: 0.5638 - val_loss: 0.8725 - val_accuracy: 0.6119 - 3s/epoch - 292ms/step\n",
            "Epoch 3/20\n",
            "11/11 - 3s - loss: 0.8417 - accuracy: 0.6439 - val_loss: 0.7391 - val_accuracy: 0.6418 - 3s/epoch - 313ms/step\n",
            "Epoch 4/20\n",
            "11/11 - 4s - loss: 0.7247 - accuracy: 0.6884 - val_loss: 0.6702 - val_accuracy: 0.6866 - 4s/epoch - 345ms/step\n",
            "Epoch 5/20\n",
            "11/11 - 4s - loss: 0.7867 - accuracy: 0.6884 - val_loss: 0.5773 - val_accuracy: 0.7164 - 4s/epoch - 352ms/step\n",
            "Epoch 6/20\n",
            "11/11 - 3s - loss: 0.5125 - accuracy: 0.8101 - val_loss: 0.4942 - val_accuracy: 0.8507 - 3s/epoch - 291ms/step\n",
            "Epoch 7/20\n",
            "11/11 - 4s - loss: 0.3960 - accuracy: 0.8457 - val_loss: 0.4236 - val_accuracy: 0.8358 - 4s/epoch - 372ms/step\n",
            "Epoch 8/20\n",
            "11/11 - 3s - loss: 0.2987 - accuracy: 0.8902 - val_loss: 0.7271 - val_accuracy: 0.6716 - 3s/epoch - 315ms/step\n",
            "Epoch 9/20\n",
            "11/11 - 3s - loss: 0.3254 - accuracy: 0.8694 - val_loss: 0.2801 - val_accuracy: 0.8209 - 3s/epoch - 295ms/step\n",
            "Epoch 10/20\n",
            "11/11 - 3s - loss: 0.2769 - accuracy: 0.8843 - val_loss: 0.2187 - val_accuracy: 0.9552 - 3s/epoch - 314ms/step\n",
            "Epoch 11/20\n",
            "11/11 - 4s - loss: 0.1731 - accuracy: 0.9436 - val_loss: 0.0757 - val_accuracy: 1.0000 - 4s/epoch - 376ms/step\n",
            "Epoch 12/20\n",
            "11/11 - 3s - loss: 0.0613 - accuracy: 0.9792 - val_loss: 0.0569 - val_accuracy: 0.9851 - 3s/epoch - 312ms/step\n",
            "Epoch 13/20\n",
            "11/11 - 3s - loss: 0.0373 - accuracy: 0.9911 - val_loss: 0.0226 - val_accuracy: 1.0000 - 3s/epoch - 295ms/step\n",
            "Epoch 14/20\n",
            "11/11 - 4s - loss: 0.0360 - accuracy: 0.9911 - val_loss: 0.1810 - val_accuracy: 0.9254 - 4s/epoch - 374ms/step\n",
            "Epoch 15/20\n",
            "11/11 - 3s - loss: 0.0678 - accuracy: 0.9792 - val_loss: 0.0623 - val_accuracy: 0.9851 - 3s/epoch - 309ms/step\n",
            "Epoch 16/20\n",
            "11/11 - 3s - loss: 0.0226 - accuracy: 0.9941 - val_loss: 0.0022 - val_accuracy: 1.0000 - 3s/epoch - 311ms/step\n",
            "Epoch 17/20\n",
            "11/11 - 4s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 8.1329e-04 - val_accuracy: 1.0000 - 4s/epoch - 365ms/step\n",
            "Epoch 18/20\n",
            "11/11 - 3s - loss: 5.8386e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 3s/epoch - 310ms/step\n",
            "Epoch 19/20\n",
            "11/11 - 3s - loss: 6.7299e-04 - accuracy: 1.0000 - val_loss: 7.3912e-04 - val_accuracy: 1.0000 - 3s/epoch - 293ms/step\n",
            "Epoch 20/20\n",
            "11/11 - 4s - loss: 4.1363e-04 - accuracy: 1.0000 - val_loss: 5.9084e-04 - val_accuracy: 1.0000 - 4s/epoch - 359ms/step\n",
            "\n",
            "Error mínimo en entrenamiento: 0.000414\n",
            "Error mínimo en validación: 0.000591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "\n",
        "# Directorio donde se encuentran las imágenes de prueba\n",
        "test_images_dir = 'test/images/'\n",
        "\n",
        "# Obtener la lista de nombres de archivos de las imágenes de prueba\n",
        "test_filenames = os.listdir(test_images_dir)\n",
        "\n",
        "# Crear un DataFrame para almacenar las predicciones\n",
        "predictions_df = pd.DataFrame({'image_filename': test_filenames})\n",
        "\n",
        "# Crear columnas para 'normal', 'benign' y 'malignant' con valores iniciales de 0\n",
        "predictions_df['normal'] = 0\n",
        "predictions_df['benign'] = 0\n",
        "predictions_df['malignant'] = 0\n",
        "\n",
        "# Iterar sobre cada imagen de prueba\n",
        "for filename in test_filenames:\n",
        "    # Cargar la imagen y preprocesarla\n",
        "    img_path = os.path.join(test_images_dir, filename)\n",
        "    img = load_img(img_path, target_size=(img_width, img_height))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Añadir una dimensión adicional para el lote\n",
        "\n",
        "    # Realizar la predicción\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = np.argmax(prediction)  # Obtener la clase predicha\n",
        "\n",
        "    # Actualizar las columnas correspondientes según la predicción\n",
        "    if predicted_class == 0:\n",
        "        predictions_df.loc[predictions_df['image_filename'] == filename, 'normal'] = 1\n",
        "    elif predicted_class == 1:\n",
        "        predictions_df.loc[predictions_df['image_filename'] == filename, 'benign'] = 1\n",
        "    elif predicted_class == 2:\n",
        "        predictions_df.loc[predictions_df['image_filename'] == filename, 'malignant'] = 1\n",
        "\n",
        "# Ordenar el DataFrame por el nombre del archivo de imagen\n",
        "predictions_df = predictions_df.sort_values(by='image_filename')\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "predictions_df.to_csv('test_predictions.csv', index=False)\n",
        "\n",
        "# Imprimir las primeras filas del DataFrame para verificar\n",
        "print(predictions_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZZRIbSOoXqN",
        "outputId": "2a4d9f04-cbc0-4a1a-8e93-b58a7f1a6a5b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 441ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "   image_filename  normal  benign  malignant\n",
            "32  image_005.png       0       0          1\n",
            "5   image_006.png       0       0          1\n",
            "45  image_007.png       0       1          0\n",
            "87  image_009.png       0       0          1\n",
            "65  image_010.png       0       0          1\n"
          ]
        }
      ]
    }
  ]
}
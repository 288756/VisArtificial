{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/288756/VisArtificial/blob/master/Hadoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación"
      ],
      "metadata": {
        "id": "wd5qH8l4SxdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar Java"
      ],
      "metadata": {
        "id": "FLOVvbWaSZtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    !apt-get install -y openjdk-11-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "ZLxu3W5-KvcI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "java_home = os.popen(\"readlink -f /usr/bin/java | sed 's:/bin/java::'\").read().strip()\n",
        "os.environ[\"JAVA_HOME\"] = java_home"
      ],
      "metadata": {
        "id": "GzJ3WprlQXv5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Java está en el path: $JAVA_HOME  (IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOm0nXPmRVQC",
        "outputId": "7dca6480-8d4e-4166-c072-0d1887daf154"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java está en el path: /usr/lib/jvm/java-11-openjdk-amd64  (IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtener la versión de Hadoop"
      ],
      "metadata": {
        "id": "IfDVZ-OI674l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "hadoop_version = os.popen(\"curl -s https://archive.apache.org/dist/hadoop/common/stable/ | grep -oP 'hadoop-*.*tar.gz\\\"' | grep --invert-match -e '-.*-' | cut -d '-' -f2 | awk -F '.tar.gz\\\"' '{print $1}'\").read().strip()\n",
        "os.environ[\"HADOOP_VERSION\"] = hadoop_version"
      ],
      "metadata": {
        "id": "YMkIqfdiIlIB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"IMPORTANTE: La versión estable de Hadoop es: $HADOOP_VERSION  (si no sale nada, hay que ponerla manualmente)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2bpWgVMJ3op",
        "outputId": "f4a212fb-db26-47c3-d7ab-eb729602ac64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTANTE: La versión estable de Hadoop es: 3.4.0  (si no sale nada, hay que ponerla manualmente)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar Hadoop"
      ],
      "metadata": {
        "id": "kkXiRMevSvVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-${HADOOP_VERSION}.tar.gz\"\n",
        "#!wget \"https://archive.apache.org/dist/hadoop/common/stable/hadoop-${HADOOP_VERSION}.tar.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGhi9B0RRpza",
        "outputId": "7d042e78-7e73-4df8-d8a3-a7efecbefeab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-05 13:07:28--  https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 965537117 (921M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.4.0.tar.gz’\n",
            "\n",
            "hadoop-3.4.0.tar.gz 100%[===================>] 920.81M   183MB/s    in 7.4s    \n",
            "\n",
            "2024-05-05 13:07:43 (124 MB/s) - ‘hadoop-3.4.0.tar.gz’ saved [965537117/965537117]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls hadoop-${HADOOP_VERSION}.tar.gz\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir hadoop-${HADOOP_VERSION}.tar.gz. Si no es así, se tendría que descargar manualmente\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vc2RgeHTCor",
        "outputId": "b65e39c5-b427-4df9-84f7-cae9aa40ec56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hadoop-3.4.0.tar.gz\n",
            "IMPORTANTE: en la anterior línea debería salir hadoop-3.4.0.tar.gz. Si no es así, se tendría que descargar manualmente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descromprimir Hadoop"
      ],
      "metadata": {
        "id": "eeYQZHGVUuqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-${HADOOP_VERSION}.tar.gz\n"
      ],
      "metadata": {
        "id": "nL0GJfuTToPp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -d hadoop-${HADOOP_VERSION}\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir la carpeta hadoop-${HADOOP_VERSION}. Si no es así, habría que descomprimirla manualmente\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELOMJ_FXTqeZ",
        "outputId": "23eb69d5-3126-4d8c-bd55-470e41903f86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hadoop-3.4.0\n",
            "IMPORTANTE: en la anterior línea debería salir la carpeta hadoop-3.4.0. Si no es así, habría que descomprimirla manualmente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mover la carpeta de Hadoop a /usr/local"
      ],
      "metadata": {
        "id": "77-DYdoGVHJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv hadoop-${HADOOP_VERSION}/ /usr/local/"
      ],
      "metadata": {
        "id": "IShD2tpHVNPC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -d /usr/local/hadoop-${HADOOP_VERSION}\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir la carpeta /usr/local/hadoop-${HADOOP_VERSION}. Si no es así, habría que moverla manualmente a esa ubicación\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJU5K-B6VVbr",
        "outputId": "26381301-5d1a-4ee3-a49c-231123e73835"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.4.0\n",
            "IMPORTANTE: en la anterior línea debería salir la carpeta /usr/local/hadoop-3.4.0. Si no es así, habría que moverla manualmente a esa ubicación\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear variable de entorno de HADOOP_HOME"
      ],
      "metadata": {
        "id": "WXuCq-HiWwvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-\" + os.environ[\"HADOOP_VERSION\"]\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\")"
      ],
      "metadata": {
        "id": "kA8dOUbDWhPE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $HADOOP_HOME\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir la carpeta de hadoop. Si no es así, habría que establecer manualmente la variable de entorno HADOOP_HOME\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbB5Za1uXJh9",
        "outputId": "518894ca-d90b-4e60-83f0-a21cc65d8d28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.4.0\n",
            "IMPORTANTE: en la anterior línea debería salir la carpeta de hadoop. Si no es así, habría que establecer manualmente la variable de entorno HADOOP_HOME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobar instalación"
      ],
      "metadata": {
        "id": "n8f3BNR9pwS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop version\n",
        "!echo \"IMPORTANTE: en las anteriores líneas debería mostrarnos la versión de Hadoop. Si no es así, habría que establecer manualmente la variable PATH añadiéndole la carpeta bin de Hadoop\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AV-bpepXTyU",
        "outputId": "9535b943-d9cb-4437-c713-3cccec699283"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.4.0\n",
            "Source code repository git@github.com:apache/hadoop.git -r bd8b77f398f626bb7791783192ee7a5dfaeec760\n",
            "Compiled by root on 2024-03-04T06:35Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.21.12\n",
            "From source with checksum f7fe694a3613358b38812ae9c31114e\n",
            "This command was run using /usr/local/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0.jar\n",
            "IMPORTANTE: en las anteriores líneas debería mostrarnos la versión de Hadoop. Si no es así, habría que establecer manualmente la variable PATH añadiéndole la carpeta bin de Hadoop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probar que puede ejecutar el programa wordcount"
      ],
      "metadata": {
        "id": "8LyZtziF2wFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear archivos de datos"
      ],
      "metadata": {
        "id": "e4TCgWVAY7wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir entradaWordCount\n",
        "cat << EOF > ./entradaWordCount/entrada-1\n",
        "Esto es una linea de prueba\n",
        "segunda linea de prueba\n",
        "Podemos incluir las líneas que queramos\n",
        "esta es la ultima linea\n",
        "EOF"
      ],
      "metadata": {
        "id": "ockfVoDy5C4U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar que el EOF marca el final del archivo. Comprobamos que se han creado los datos en la ubicación ./entradaWordCount/entrada-1"
      ],
      "metadata": {
        "id": "WwNJxUKQZDRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./entradaWordCount/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPiYAyWNZAN2",
        "outputId": "a08365c9-b5df-449a-baa3-53444e22a45f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 117 May  5 13:17 ./entradaWordCount/entrada-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos su contenido"
      ],
      "metadata": {
        "id": "XDj33Hk8ZPJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./entradaWordCount/entrada-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejlnbI9_ZM38",
        "outputId": "0300fb67-d5a7-43ae-d56c-3b903249b81d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esto es una linea de prueba\n",
            "segunda linea de prueba\n",
            "Podemos incluir las líneas que queramos\n",
            "esta es la ultima linea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar programa"
      ],
      "metadata": {
        "id": "YsAR_qO0ZcKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar wordcount ./entradaWordCount ./salidaWordCount\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB6yf5fGZTZg",
        "outputId": "001be9c8-a26b-40cf-ffea-0aba674befc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-05 13:17:23,651 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-05 13:17:23,963 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-05 13:17:23,964 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-05 13:17:24,415 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2024-05-05 13:17:24,489 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-05 13:17:25,010 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local133033416_0001\n",
            "2024-05-05 13:17:25,010 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-05 13:17:25,305 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-05 13:17:25,306 INFO mapreduce.Job: Running job: job_local133033416_0001\n",
            "2024-05-05 13:17:25,317 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-05 13:17:25,328 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2024-05-05 13:17:25,329 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-05 13:17:25,330 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-05 13:17:25,334 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2024-05-05 13:17:25,414 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-05 13:17:25,416 INFO mapred.LocalJobRunner: Starting task: attempt_local133033416_0001_m_000000_0\n",
            "2024-05-05 13:17:25,478 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2024-05-05 13:17:25,481 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-05 13:17:25,486 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-05 13:17:25,549 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-05 13:17:25,569 INFO mapred.MapTask: Processing split: file:/content/entradaWordCount/entrada-1:0+117\n",
            "2024-05-05 13:17:25,782 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-05 13:17:25,782 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-05 13:17:25,782 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-05 13:17:25,782 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-05 13:17:25,782 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-05 13:17:25,792 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-05 13:17:25,817 INFO mapred.LocalJobRunner: \n",
            "2024-05-05 13:17:25,826 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-05 13:17:25,826 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-05 13:17:25,827 INFO mapred.MapTask: bufstart = 0; bufend = 201; bufvoid = 104857600\n",
            "2024-05-05 13:17:25,827 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214316(104857264); length = 81/6553600\n",
            "2024-05-05 13:17:25,860 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-05 13:17:25,906 INFO mapred.Task: Task:attempt_local133033416_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-05 13:17:25,912 INFO mapred.LocalJobRunner: map\n",
            "2024-05-05 13:17:25,913 INFO mapred.Task: Task 'attempt_local133033416_0001_m_000000_0' done.\n",
            "2024-05-05 13:17:25,937 INFO mapred.Task: Final Counters for attempt_local133033416_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=281885\n",
            "\t\tFILE: Number of bytes written=992311\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4\n",
            "\t\tMap output records=21\n",
            "\t\tMap output bytes=201\n",
            "\t\tMap output materialized bytes=194\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=21\n",
            "\t\tCombine output records=16\n",
            "\t\tSpilled Records=16\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=20\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=117\n",
            "2024-05-05 13:17:25,937 INFO mapred.LocalJobRunner: Finishing task: attempt_local133033416_0001_m_000000_0\n",
            "2024-05-05 13:17:25,940 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-05 13:17:25,945 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-05 13:17:25,946 INFO mapred.LocalJobRunner: Starting task: attempt_local133033416_0001_r_000000_0\n",
            "2024-05-05 13:17:25,963 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2024-05-05 13:17:25,963 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-05 13:17:25,963 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-05 13:17:25,965 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-05 13:17:25,969 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@19dc6233\n",
            "2024-05-05 13:17:25,972 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-05 13:17:26,006 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-05 13:17:26,009 INFO reduce.EventFetcher: attempt_local133033416_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-05 13:17:26,057 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local133033416_0001_m_000000_0 decomp: 190 len: 194 to MEMORY\n",
            "2024-05-05 13:17:26,063 INFO reduce.InMemoryMapOutput: Read 190 bytes from map-output for attempt_local133033416_0001_m_000000_0\n",
            "2024-05-05 13:17:26,065 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 190, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->190\n",
            "2024-05-05 13:17:26,070 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-05 13:17:26,071 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-05 13:17:26,071 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-05 13:17:26,079 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-05 13:17:26,079 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 183 bytes\n",
            "2024-05-05 13:17:26,081 INFO reduce.MergeManagerImpl: Merged 1 segments, 190 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-05 13:17:26,082 INFO reduce.MergeManagerImpl: Merging 1 files, 194 bytes from disk\n",
            "2024-05-05 13:17:26,082 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-05 13:17:26,083 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-05 13:17:26,084 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 183 bytes\n",
            "2024-05-05 13:17:26,084 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-05 13:17:26,088 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-05 13:17:26,092 INFO mapred.Task: Task:attempt_local133033416_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-05 13:17:26,094 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-05 13:17:26,094 INFO mapred.Task: Task attempt_local133033416_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-05 13:17:26,096 INFO output.FileOutputCommitter: Saved output of task 'attempt_local133033416_0001_r_000000_0' to file:/content/salidaWordCount\n",
            "2024-05-05 13:17:26,097 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-05-05 13:17:26,097 INFO mapred.Task: Task 'attempt_local133033416_0001_r_000000_0' done.\n",
            "2024-05-05 13:17:26,098 INFO mapred.Task: Final Counters for attempt_local133033416_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282305\n",
            "\t\tFILE: Number of bytes written=992641\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=16\n",
            "\t\tReduce shuffle bytes=194\n",
            "\t\tReduce input records=16\n",
            "\t\tReduce output records=16\n",
            "\t\tSpilled Records=16\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=136\n",
            "2024-05-05 13:17:26,098 INFO mapred.LocalJobRunner: Finishing task: attempt_local133033416_0001_r_000000_0\n",
            "2024-05-05 13:17:26,098 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-05 13:17:26,315 INFO mapreduce.Job: Job job_local133033416_0001 running in uber mode : false\n",
            "2024-05-05 13:17:26,316 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-05 13:17:26,318 INFO mapreduce.Job: Job job_local133033416_0001 completed successfully\n",
            "2024-05-05 13:17:26,331 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=564190\n",
            "\t\tFILE: Number of bytes written=1984952\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4\n",
            "\t\tMap output records=21\n",
            "\t\tMap output bytes=201\n",
            "\t\tMap output materialized bytes=194\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=21\n",
            "\t\tCombine output records=16\n",
            "\t\tReduce input groups=16\n",
            "\t\tReduce shuffle bytes=194\n",
            "\t\tReduce input records=16\n",
            "\t\tReduce output records=16\n",
            "\t\tSpilled Records=32\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=20\n",
            "\t\tTotal committed heap usage (bytes)=717225984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=117\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos que se ha creado una carpeta ./salidaWordCount con la salida"
      ],
      "metadata": {
        "id": "9jtFES0vZoQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./salidaWordCount\n",
        "!echo \"IMPORTANTE: si no sale nada en las líneas anteriores, es que el programa no se ha ejecutado correctamente\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oviJA6RZldt",
        "outputId": "4602b3d6-c73a-464c-add1-f5878074a224"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-r-00000  _SUCCESS\n",
            "IMPORTANTE: si no sale nada en las líneas anteriores, es que el programa no se ha ejecutado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abrimos la salida"
      ],
      "metadata": {
        "id": "8gVSDN6yZ4Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./salidaWordCount/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7US45oltZvlV",
        "outputId": "36c09c01-e84f-4c3e-af4d-9a519f24f7ea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esto\t1\n",
            "Podemos\t1\n",
            "de\t2\n",
            "es\t2\n",
            "esta\t1\n",
            "incluir\t1\n",
            "la\t1\n",
            "las\t1\n",
            "linea\t3\n",
            "líneas\t1\n",
            "prueba\t2\n",
            "que\t1\n",
            "queramos\t1\n",
            "segunda\t1\n",
            "ultima\t1\n",
            "una\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear y ejecutar un programa con python"
      ],
      "metadata": {
        "id": "j1l3qZzj1Fmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear el archivo Mapper.py"
      ],
      "metadata": {
        "id": "HcxGE-0M1K-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOF > Mapper.py\n",
        "#!/usr/bin/python3\n",
        "import sys\n",
        "\n",
        "\n",
        "'''\n",
        "Mapper de MenosMil\n",
        "Creado por Javier Braga\n",
        "'''\n",
        "#Por cada medida de temp emitimos los pares <cliente, dineroGastado>\n",
        "for linea in sys.stdin:\n",
        "      linea = linea.strip()\n",
        "      cliente, dineroGastado = linea.split(\"  \", 2)\n",
        "      print(\"%s\\t%s\" % (cliente, dineroGastado))\n",
        "\n",
        "#Finaliza el Mapper\n",
        "EOF"
      ],
      "metadata": {
        "id": "Q61gD90O1XX6"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear el archivo Reducer.py"
      ],
      "metadata": {
        "id": "eY5NTqI62pdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOF > Reducer.py\n",
        "#!/usr/bin/python3\n",
        "import sys\n",
        "\n",
        "'''\n",
        "Reducer de MenosMil\n",
        "Creado por Javier Braga\n",
        "'''\n",
        "\n",
        "subproblema = None\n",
        "cantMaxima = 1000\n",
        "\n",
        "for claveValor in sys.stdin:\n",
        "    # Dividir la entrada en cliente y dinero gastado\n",
        "    cliente, dineroGastado = claveValor.split(\"\\t\", 1)\n",
        "\n",
        "    # Convertir el dinero gastado a float\n",
        "    dineroGastado = int(dineroGastado)\n",
        "\n",
        "    # Si el cliente es el mismo que el subproblema actual, restar el dinero gastado\n",
        "    if subproblema == cliente:\n",
        "        cantMaxima -= dineroGastado\n",
        "\n",
        "    else: # Si hemos terminado con el subproblema actual, emitir resultado y actualizar subproblema\n",
        "        if subproblema:\n",
        "            if cantMaxima > 0:\n",
        "                print(subproblema)\n",
        "        # Actualiza el cliente actual y la cantidad máxima\n",
        "        subproblema = cliente\n",
        "        cantMaxima = 1000 - dineroGastado\n",
        "# El bucle anterior no emite el último subproblema\n",
        "if subproblema:\n",
        "    if cantMaxima > 0:\n",
        "        print(subproblema)\n",
        "EOF"
      ],
      "metadata": {
        "id": "PpqTdgZX10BG"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear los datos de entrada"
      ],
      "metadata": {
        "id": "jASBQ4-b2rwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOF > medidas.txt\n",
        "Joel  1100\n",
        "Iris  400\n",
        "Virgilio  200\n",
        "Adelia  400\n",
        "Cristina  235\n",
        "Ema  235\n",
        "Virgilio  480\n",
        "Soraya  350\n",
        "Ema  750\n",
        "Asuncion  450\n",
        "Joel  200\n",
        "Corona  50\n",
        "Cristina  600\n",
        "Haydee  300\n",
        "Haydee  200\n",
        "Haydee  200\n",
        "Adelia  600\n",
        "EOF"
      ],
      "metadata": {
        "id": "EgghcpeV2UI5"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar que el EOF marca el final del archivo"
      ],
      "metadata": {
        "id": "hZQZnboS3eRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar programa"
      ],
      "metadata": {
        "id": "G-WQ7WRW3kuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-${HADOOP_VERSION}.jar -files ./Mapper.py,./Reducer.py -mapper ./Mapper.py -reducer ./Reducer.py -input medidas.txt -output ./salidaMenosMil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GwomN8K2gyy",
        "outputId": "60d9c8c4-548f-41b3-b0c6-62c289e8048a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-05 14:28:20,931 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-05-05 14:28:21,155 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-05-05 14:28:21,156 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-05-05 14:28:21,195 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-05 14:28:21,585 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-05 14:28:21,617 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-05 14:28:22,000 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local263696387_0001\n",
            "2024-05-05 14:28:22,000 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-05 14:28:22,491 INFO mapred.LocalDistributedCacheManager: Localized file:/content/Mapper.py as file:/tmp/hadoop-root/mapred/local/job_local263696387_0001_2e9bcc6c-dda5-44c8-afc9-985dc4b2c027/Mapper.py\n",
            "2024-05-05 14:28:22,532 INFO mapred.LocalDistributedCacheManager: Localized file:/content/Reducer.py as file:/tmp/hadoop-root/mapred/local/job_local263696387_0001_06c0d6ed-c62d-4501-a7e5-5e6cceaa705f/Reducer.py\n",
            "2024-05-05 14:28:22,676 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-05-05 14:28:22,680 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-05-05 14:28:22,681 INFO mapreduce.Job: Running job: job_local263696387_0001\n",
            "2024-05-05 14:28:22,686 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-05-05 14:28:22,693 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-05 14:28:22,693 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-05 14:28:22,765 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-05-05 14:28:22,770 INFO mapred.LocalJobRunner: Starting task: attempt_local263696387_0001_m_000000_0\n",
            "2024-05-05 14:28:22,813 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-05 14:28:22,814 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-05 14:28:22,833 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-05 14:28:22,846 INFO mapred.MapTask: Processing split: file:/content/medidas.txt:0+202\n",
            "2024-05-05 14:28:22,864 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-05-05 14:28:22,952 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-05-05 14:28:22,952 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-05-05 14:28:22,952 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-05-05 14:28:22,952 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-05-05 14:28:22,952 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-05-05 14:28:22,963 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-05-05 14:28:22,974 INFO streaming.PipeMapRed: PipeMapRed exec [/content/././Mapper.py]\n",
            "2024-05-05 14:28:22,982 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-05-05 14:28:22,986 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-05-05 14:28:22,986 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-05-05 14:28:22,987 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-05-05 14:28:22,987 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-05-05 14:28:22,988 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-05-05 14:28:22,990 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-05-05 14:28:22,990 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-05-05 14:28:22,991 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-05-05 14:28:22,994 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-05-05 14:28:22,995 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-05-05 14:28:22,996 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-05-05 14:28:23,031 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-05 14:28:23,032 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-05 14:28:23,075 INFO streaming.PipeMapRed: Records R/W=17/1\n",
            "2024-05-05 14:28:23,079 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-05 14:28:23,082 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-05 14:28:23,087 INFO mapred.LocalJobRunner: \n",
            "2024-05-05 14:28:23,087 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-05-05 14:28:23,087 INFO mapred.MapTask: Spilling map output\n",
            "2024-05-05 14:28:23,087 INFO mapred.MapTask: bufstart = 0; bufend = 185; bufvoid = 104857600\n",
            "2024-05-05 14:28:23,087 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214332(104857328); length = 65/6553600\n",
            "2024-05-05 14:28:23,096 INFO mapred.MapTask: Finished spill 0\n",
            "2024-05-05 14:28:23,115 INFO mapred.Task: Task:attempt_local263696387_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-05-05 14:28:23,125 INFO mapred.LocalJobRunner: Records R/W=17/1\n",
            "2024-05-05 14:28:23,126 INFO mapred.Task: Task 'attempt_local263696387_0001_m_000000_0' done.\n",
            "2024-05-05 14:28:23,137 INFO mapred.Task: Final Counters for attempt_local263696387_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=143365\n",
            "\t\tFILE: Number of bytes written=859873\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=17\n",
            "\t\tMap output records=17\n",
            "\t\tMap output bytes=185\n",
            "\t\tMap output materialized bytes=225\n",
            "\t\tInput split bytes=77\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=17\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=416284672\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=202\n",
            "2024-05-05 14:28:23,137 INFO mapred.LocalJobRunner: Finishing task: attempt_local263696387_0001_m_000000_0\n",
            "2024-05-05 14:28:23,137 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-05-05 14:28:23,142 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-05-05 14:28:23,142 INFO mapred.LocalJobRunner: Starting task: attempt_local263696387_0001_r_000000_0\n",
            "2024-05-05 14:28:23,157 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-05-05 14:28:23,157 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-05-05 14:28:23,158 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-05-05 14:28:23,165 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@374f676e\n",
            "2024-05-05 14:28:23,169 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-05-05 14:28:23,200 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-05-05 14:28:23,206 INFO reduce.EventFetcher: attempt_local263696387_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-05-05 14:28:23,289 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local263696387_0001_m_000000_0 decomp: 221 len: 225 to MEMORY\n",
            "2024-05-05 14:28:23,305 INFO reduce.InMemoryMapOutput: Read 221 bytes from map-output for attempt_local263696387_0001_m_000000_0\n",
            "2024-05-05 14:28:23,311 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 221, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->221\n",
            "2024-05-05 14:28:23,316 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-05-05 14:28:23,318 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-05 14:28:23,318 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-05-05 14:28:23,331 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-05 14:28:23,331 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 212 bytes\n",
            "2024-05-05 14:28:23,333 INFO reduce.MergeManagerImpl: Merged 1 segments, 221 bytes to disk to satisfy reduce memory limit\n",
            "2024-05-05 14:28:23,334 INFO reduce.MergeManagerImpl: Merging 1 files, 225 bytes from disk\n",
            "2024-05-05 14:28:23,335 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-05-05 14:28:23,335 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-05-05 14:28:23,336 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 212 bytes\n",
            "2024-05-05 14:28:23,337 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-05 14:28:23,343 INFO streaming.PipeMapRed: PipeMapRed exec [/content/././Reducer.py]\n",
            "2024-05-05 14:28:23,347 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-05-05 14:28:23,351 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-05-05 14:28:23,381 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-05 14:28:23,382 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-05-05 14:28:23,418 INFO streaming.PipeMapRed: Records R/W=17/1\n",
            "2024-05-05 14:28:23,423 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-05-05 14:28:23,424 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-05-05 14:28:23,425 INFO mapred.Task: Task:attempt_local263696387_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-05-05 14:28:23,427 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-05-05 14:28:23,428 INFO mapred.Task: Task attempt_local263696387_0001_r_000000_0 is allowed to commit now\n",
            "2024-05-05 14:28:23,430 INFO output.FileOutputCommitter: Saved output of task 'attempt_local263696387_0001_r_000000_0' to file:/content/salidaMenosMil\n",
            "2024-05-05 14:28:23,431 INFO mapred.LocalJobRunner: Records R/W=17/1 > reduce\n",
            "2024-05-05 14:28:23,431 INFO mapred.Task: Task 'attempt_local263696387_0001_r_000000_0' done.\n",
            "2024-05-05 14:28:23,432 INFO mapred.Task: Final Counters for attempt_local263696387_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=143847\n",
            "\t\tFILE: Number of bytes written=860175\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10\n",
            "\t\tReduce shuffle bytes=225\n",
            "\t\tReduce input records=17\n",
            "\t\tReduce output records=8\n",
            "\t\tSpilled Records=17\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=416284672\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=77\n",
            "2024-05-05 14:28:23,432 INFO mapred.LocalJobRunner: Finishing task: attempt_local263696387_0001_r_000000_0\n",
            "2024-05-05 14:28:23,432 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-05-05 14:28:23,688 INFO mapreduce.Job: Job job_local263696387_0001 running in uber mode : false\n",
            "2024-05-05 14:28:23,689 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-05 14:28:23,691 INFO mapreduce.Job: Job job_local263696387_0001 completed successfully\n",
            "2024-05-05 14:28:23,700 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=287212\n",
            "\t\tFILE: Number of bytes written=1720048\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=17\n",
            "\t\tMap output records=17\n",
            "\t\tMap output bytes=185\n",
            "\t\tMap output materialized bytes=225\n",
            "\t\tInput split bytes=77\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10\n",
            "\t\tReduce shuffle bytes=225\n",
            "\t\tReduce input records=17\n",
            "\t\tReduce output records=8\n",
            "\t\tSpilled Records=34\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=832569344\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=202\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=77\n",
            "2024-05-05 14:28:23,700 INFO streaming.StreamJob: Output directory: ./salidaMenosMil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobar salida"
      ],
      "metadata": {
        "id": "sf4whLva4qV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos que se ha creado la carpeta ./salidaMaxTemp con el resultado"
      ],
      "metadata": {
        "id": "kriuXS9x4zo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./salidaMenosMil/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo_uBc3F3UzC",
        "outputId": "a0644aac-b2f6-41d8-e307-7c577ac0dda6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 65 May  5 14:28 part-00000\n",
            "-rw-r--r-- 1 root root  0 May  5 14:28 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abrimos el archivo que contiene la salida"
      ],
      "metadata": {
        "id": "gA1iqmvh4yBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./salidaMenosMil/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_gWADNU4wl8",
        "outputId": "58c8c15f-8821-43ed-b779-4420df64da5d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asuncion\t\n",
            "Corona\t\n",
            "Cristina\t\n",
            "Ema\t\n",
            "Haydee\t\n",
            "Iris\t\n",
            "Soraya\t\n",
            "Virgilio\t\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/288756/VisArtificial/blob/master/Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalación"
      ],
      "metadata": {
        "id": "nQ3IDy73inU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instalar Java"
      ],
      "metadata": {
        "id": "_tLpSnhvj9rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "9-dBWDVui435"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "java_home = os.popen(\"readlink -f /usr/bin/java | sed 's:/bin/java::'\").read().strip()\n",
        "os.environ[\"JAVA_HOME\"] = java_home"
      ],
      "metadata": {
        "id": "GzJ3WprlQXv5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Java está en el path: $JAVA_HOME  (IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOm0nXPmRVQC",
        "outputId": "e6147667-39c5-486d-b5d3-66b1cb3c4373"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java está en el path: /usr/lib/jvm/java-11-openjdk-amd64  (IMPORTANTE: si no sale nada o no es la versión correcta, hay que instalarla y cambiarla manualmente)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtener versión de Spark"
      ],
      "metadata": {
        "id": "7L74V8vRkDNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "spark_version = os.popen(\"curl -s https://archive.apache.org/dist/spark/ | grep -oP 'spark-*.*/\\\"' | grep --invert-match -e '-.*-' | awk -F '/\\\"' '{print $1}' | cut -d '-' -f2 | tail -n 1\").read().strip()\n",
        "os.environ[\"SPARK_VERSION\"] = spark_version"
      ],
      "metadata": {
        "id": "Kfxo9uvekAZ8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"IMPORTANTE: La versión estable de Spark es: $SPARK_VERSION  (si no sale nada, hay que ponerla manualmente)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTz8Q48gmN7r",
        "outputId": "8626bc95-5670-4009-de9e-d2b6c2caa2a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTANTE: La versión estable de Spark es: 3.5.1  (si no sale nada, hay que ponerla manualmente)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar Spark"
      ],
      "metadata": {
        "id": "e0PG_cQNkBmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz\"\n",
        "#!wget \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puh9N5SDmWis",
        "outputId": "cbdea528-b975-41c3-85ef-56867dca184d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-16 14:15:38--  https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400446614 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.1-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.1-bin-had 100%[===================>] 381.90M  17.2MB/s    in 5.4s    \n",
            "\n",
            "2024-04-16 14:15:58 (71.2 MB/s) - ‘spark-3.5.1-bin-hadoop3.tgz’ saved [400446614/400446614]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls spark-${SPARK_VERSION}-bin-hadoop3.tgz\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir spark-${SPARK_VERSION}-bin-hadoop3.tgz. Si no es así, se tendría que descargar manualmente\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5ZsdscKm1c_",
        "outputId": "85b4932a-2acd-4bf0-a053-b82f03a94492"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.5.1-bin-hadoop3.tgz\n",
            "IMPORTANTE: en la anterior línea debería salir spark-3.5.1-bin-hadoop3.tgz. Si no es así, se tendría que descargar manualmente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Descomprimir Spark"
      ],
      "metadata": {
        "id": "99a2TZlPnB9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "ySAVzrwHnPXt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -d spark-${SPARK_VERSION}-bin-hadoop3\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir la carpeta spark-${SPARK_VERSION}-bin-hadoop3. Si no es así, habría que descomprimirla manualmente\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkuUsKKMnTPr",
        "outputId": "e046763b-34f1-481f-ebda-4f27ec710eea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.5.1-bin-hadoop3\n",
            "IMPORTANTE: en la anterior línea debería salir la carpeta spark-3.5.1-bin-hadoop3. Si no es así, habría que descomprimirla manualmente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mover la carpeta de Spark a /usr/local"
      ],
      "metadata": {
        "id": "vDPuTI-ZnY7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv spark-${SPARK_VERSION}-bin-hadoop3 /usr/local/"
      ],
      "metadata": {
        "id": "Favb4xaSndC7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -d /usr/local/spark-${SPARK_VERSION}-bin-hadoop3\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir la carpeta /usr/local/spark-${SPARK_VERSION}-bin-hadoop3. Si no es así, habría que moverla manualmente a esa ubicación\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otH2OQgQnpEA",
        "outputId": "f1ea1992-d21e-404d-be32-807ec00509e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/spark-3.5.1-bin-hadoop3\n",
            "IMPORTANTE: en la anterior línea debería salir la carpeta /usr/local/spark-3.5.1-bin-hadoop3. Si no es así, habría que moverla manualmente a esa ubicación\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear variable de entorno de SPARK_HOME"
      ],
      "metadata": {
        "id": "XruATfO6nsuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark-\" + os.environ[\"SPARK_VERSION\"] + \"-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"SPARK_HOME\"], \"bin\")"
      ],
      "metadata": {
        "id": "c37poWJ3nx66"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $SPARK_HOME\n",
        "!echo \"IMPORTANTE: en la anterior línea debería salir la carpeta de spark. Si no es así, habría que establecer manualmente la variable de entorno SPARK_HOME\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etTpsv84pmxU",
        "outputId": "6e0baa6a-c9df-4623-d340-2096b15143cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/spark-3.5.1-bin-hadoop3\n",
            "IMPORTANTE: en la anterior línea debería salir la carpeta de spark. Si no es así, habría que establecer manualmente la variable de entorno SPARK_HOME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comprobar instalación"
      ],
      "metadata": {
        "id": "Fcl92G7fpy7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --version\n",
        "!echo \"IMPORTANTE: en las anteriores líneas debería mostrarnos la versión de Spark. Si no es así, habría que establecer manualmente la variable PATH añadiéndole la carpeta bin de Spark\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V94t_71Cpqlz",
        "outputId": "592a5ae1-64bf-4c33-8101-81604a0baa7c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.22\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n",
            "IMPORTANTE: en las anteriores líneas debería mostrarnos la versión de Spark. Si no es así, habría que establecer manualmente la variable PATH añadiéndole la carpeta bin de Spark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar PySpark"
      ],
      "metadata": {
        "id": "ulBVazaEI4j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGtYMHeKI9rR",
        "outputId": "dad9d0e7-ac56-43e8-9df7-603efc8ffa05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=0115cefb40f52cfb519fc9dc865f1728e0f13f52e8052ee43098cceb0add3f99\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejecución interactiva"
      ],
      "metadata": {
        "id": "Zy9oLSSkHY14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el contexto"
      ],
      "metadata": {
        "id": "n97fGVruJImg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"\")\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "YtQy_R6Yp2cT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos interactivamente el código"
      ],
      "metadata": {
        "id": "AuVxuA2cJWCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([\"hola mundo\"]).map(lambda record : record.split(\" \")).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpuzB9OsJY1e",
        "outputId": "6de84cc2-6038-4a5c-8600-24f3cc01a964"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hola', 'mundo']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejecución de programa"
      ],
      "metadata": {
        "id": "g06QCjDVIW-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear el programa miPrograma.py"
      ],
      "metadata": {
        "id": "4aaWGRicJpFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOF > miPrograma.py\n",
        "#!/usr/bin/python3\n",
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "'''\n",
        "Programa creado por Jesus Moran\n",
        "Este programa cuenta el numero de apariciones de cada palabra\n",
        "'''\n",
        "\n",
        "#inicializacion\n",
        "spark = SparkSession.builder.appName('miYTCount').getOrCreate()\n",
        "\n",
        "entrada = sys.argv[1]\n",
        "salida = sys.argv[2]\n",
        "\n",
        "#cargamos los datos de entrada\n",
        "datosEntrada = spark.sparkContext.textFile(entrada)\n",
        "\n",
        "\n",
        "#hacemos el conteo de cada palabra\n",
        "conteo = datosEntrada.flatMap(lambda linea: linea.split(\" \")).map(lambda palabra: (palabra, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "\n",
        "#guardamos la salida\n",
        "conteo.saveAsTextFile(salida)\n",
        "\n",
        "#Finaliza el programa\n",
        "EOF"
      ],
      "metadata": {
        "id": "yaCw3ySkIXIp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar que el EOF marca el final del archivo (del programa)"
      ],
      "metadata": {
        "id": "ScTW6RTNKWHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear los datos de entrada"
      ],
      "metadata": {
        "id": "CahFsA1RIf24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOF > miEntrada\n",
        "a a a Musica  a 100 a a a a\n",
        "a a a Juegos  a 125 a a a a\n",
        "a a a Deportes  a 270 a a a a\n",
        "a a a Vlogs  a 210 a a a a\n",
        "a a a Musica  a 240 a a a a\n",
        "a a a Juegos  a 50 a a a a\n",
        "a a a Ciencia  a 75 a a a a\n",
        "a a a Ciencia  a 195 a a a a\n",
        "a a a Deportes  a 45 a a a a\n",
        "a a a Ciencia  a 15 a a a a\n",
        "a a a Musica  a 205 a a a a\n",
        "EOF"
      ],
      "metadata": {
        "id": "I1QDuRNKIgcj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notar que el EOF marca el final del archivo"
      ],
      "metadata": {
        "id": "UayFVKuwKUzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar programa"
      ],
      "metadata": {
        "id": "SI2t2rGKKdHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit miPrograma.py miEntrada miSalida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io8iZtnsKSbk",
        "outputId": "406857d6-2502-4e92-f774-4dbc7ae44951"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/04/16 14:19:36 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/04/16 14:19:36 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "24/04/16 14:19:36 INFO SparkContext: Java version 11.0.22\n",
            "24/04/16 14:19:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/04/16 14:19:36 INFO ResourceUtils: ==============================================================\n",
            "24/04/16 14:19:36 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/04/16 14:19:36 INFO ResourceUtils: ==============================================================\n",
            "24/04/16 14:19:36 INFO SparkContext: Submitted application: miWordCount\n",
            "24/04/16 14:19:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/04/16 14:19:36 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/04/16 14:19:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/04/16 14:19:36 INFO SecurityManager: Changing view acls to: root\n",
            "24/04/16 14:19:36 INFO SecurityManager: Changing modify acls to: root\n",
            "24/04/16 14:19:36 INFO SecurityManager: Changing view acls groups to: \n",
            "24/04/16 14:19:36 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/04/16 14:19:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/04/16 14:19:37 INFO Utils: Successfully started service 'sparkDriver' on port 37887.\n",
            "24/04/16 14:19:37 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/04/16 14:19:37 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/04/16 14:19:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/04/16 14:19:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/04/16 14:19:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/04/16 14:19:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b3af5247-2416-4224-9d8b-54083ea1780c\n",
            "24/04/16 14:19:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/04/16 14:19:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/04/16 14:19:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/04/16 14:19:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "24/04/16 14:19:37 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "24/04/16 14:19:37 INFO Executor: Starting executor ID driver on host e22968259035\n",
            "24/04/16 14:19:37 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "24/04/16 14:19:37 INFO Executor: Java version 11.0.22\n",
            "24/04/16 14:19:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/04/16 14:19:38 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@462e6f45 for default.\n",
            "24/04/16 14:19:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43139.\n",
            "24/04/16 14:19:38 INFO NettyBlockTransferService: Server created on e22968259035:43139\n",
            "24/04/16 14:19:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/04/16 14:19:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e22968259035, 43139, None)\n",
            "24/04/16 14:19:38 INFO BlockManagerMasterEndpoint: Registering block manager e22968259035:43139 with 434.4 MiB RAM, BlockManagerId(driver, e22968259035, 43139, None)\n",
            "24/04/16 14:19:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e22968259035, 43139, None)\n",
            "24/04/16 14:19:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e22968259035, 43139, None)\n",
            "24/04/16 14:19:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
            "24/04/16 14:19:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
            "24/04/16 14:19:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e22968259035:43139 (size: 32.6 KiB, free: 434.4 MiB)\n",
            "24/04/16 14:19:39 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/04/16 14:19:40 INFO FileInputFormat: Total input files to process : 1\n",
            "24/04/16 14:19:40 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "24/04/16 14:19:40 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/04/16 14:19:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/04/16 14:19:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/04/16 14:19:40 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "24/04/16 14:19:40 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/miPrograma.py:21) as input to shuffle 0\n",
            "24/04/16 14:19:40 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n",
            "24/04/16 14:19:40 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)\n",
            "24/04/16 14:19:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "24/04/16 14:19:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "24/04/16 14:19:40 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/miPrograma.py:21), which has no missing parents\n",
            "24/04/16 14:19:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 434.1 MiB)\n",
            "24/04/16 14:19:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.1 MiB)\n",
            "24/04/16 14:19:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e22968259035:43139 (size: 7.6 KiB, free: 434.4 MiB)\n",
            "24/04/16 14:19:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/04/16 14:19:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/miPrograma.py:21) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/04/16 14:19:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "24/04/16 14:19:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e22968259035, executor driver, partition 0, PROCESS_LOCAL, 7643 bytes) \n",
            "24/04/16 14:19:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (e22968259035, executor driver, partition 1, PROCESS_LOCAL, 7643 bytes) \n",
            "24/04/16 14:19:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/04/16 14:19:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/04/16 14:19:41 INFO HadoopRDD: Input split: file:/content/miEntrada:66+66\n",
            "24/04/16 14:19:41 INFO HadoopRDD: Input split: file:/content/miEntrada:0+66\n",
            "24/04/16 14:19:42 INFO PythonRunner: Times: total = 1112, boot = 842, init = 269, finish = 1\n",
            "24/04/16 14:19:42 INFO PythonRunner: Times: total = 1211, boot = 849, init = 361, finish = 1\n",
            "24/04/16 14:19:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1624 bytes result sent to driver\n",
            "24/04/16 14:19:42 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1624 bytes result sent to driver\n",
            "24/04/16 14:19:42 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2010 ms on e22968259035 (executor driver) (1/2)\n",
            "24/04/16 14:19:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2145 ms on e22968259035 (executor driver) (2/2)\n",
            "24/04/16 14:19:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/04/16 14:19:42 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33849\n",
            "24/04/16 14:19:42 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/miPrograma.py:21) finished in 2.425 s\n",
            "24/04/16 14:19:42 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/04/16 14:19:42 INFO DAGScheduler: running: Set()\n",
            "24/04/16 14:19:42 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "24/04/16 14:19:42 INFO DAGScheduler: failed: Set()\n",
            "24/04/16 14:19:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "24/04/16 14:19:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 108.5 KiB, free 434.0 MiB)\n",
            "24/04/16 14:19:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 41.0 KiB, free 434.0 MiB)\n",
            "24/04/16 14:19:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e22968259035:43139 (size: 41.0 KiB, free: 434.3 MiB)\n",
            "24/04/16 14:19:42 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "24/04/16 14:19:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/04/16 14:19:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "24/04/16 14:19:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (e22968259035, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
            "24/04/16 14:19:42 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (e22968259035, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
            "24/04/16 14:19:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "24/04/16 14:19:42 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "24/04/16 14:19:43 INFO ShuffleBlockFetcherIterator: Getting 2 (235.0 B) non-empty blocks including 2 (235.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/04/16 14:19:43 INFO ShuffleBlockFetcherIterator: Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/04/16 14:19:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms\n",
            "24/04/16 14:19:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "24/04/16 14:19:43 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/04/16 14:19:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/04/16 14:19:43 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/04/16 14:19:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/04/16 14:19:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/04/16 14:19:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/04/16 14:19:43 INFO PythonRunner: Times: total = 286, boot = -985, init = 1271, finish = 0\n",
            "24/04/16 14:19:43 INFO FileOutputCommitter: Saved output of task 'attempt_20240416141940583202085059931867_0008_m_000000_0' to file:/content/miSalida/_temporary/0/task_20240416141940583202085059931867_0008_m_000000\n",
            "24/04/16 14:19:43 INFO SparkHadoopMapRedUtil: attempt_20240416141940583202085059931867_0008_m_000000_0: Committed. Elapsed time: 2 ms.\n",
            "24/04/16 14:19:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2394 bytes result sent to driver\n",
            "24/04/16 14:19:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 616 ms on e22968259035 (executor driver) (1/2)\n",
            "24/04/16 14:19:43 INFO PythonRunner: Times: total = 410, boot = -861, init = 1271, finish = 0\n",
            "24/04/16 14:19:43 INFO FileOutputCommitter: Saved output of task 'attempt_20240416141940583202085059931867_0008_m_000001_0' to file:/content/miSalida/_temporary/0/task_20240416141940583202085059931867_0008_m_000001\n",
            "24/04/16 14:19:43 INFO SparkHadoopMapRedUtil: attempt_20240416141940583202085059931867_0008_m_000001_0: Committed. Elapsed time: 9 ms.\n",
            "24/04/16 14:19:43 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver\n",
            "24/04/16 14:19:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 691 ms on e22968259035 (executor driver) (2/2)\n",
            "24/04/16 14:19:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/04/16 14:19:43 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:83) finished in 0.759 s\n",
            "24/04/16 14:19:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/04/16 14:19:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "24/04/16 14:19:43 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 3.374447 s\n",
            "24/04/16 14:19:43 INFO SparkHadoopWriter: Start to commit write Job job_20240416141940583202085059931867_0008.\n",
            "24/04/16 14:19:43 INFO SparkHadoopWriter: Write Job job_20240416141940583202085059931867_0008 committed. Elapsed time: 24 ms.\n",
            "24/04/16 14:19:43 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/04/16 14:19:43 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/04/16 14:19:43 INFO SparkUI: Stopped Spark web UI at http://e22968259035:4041\n",
            "24/04/16 14:19:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/04/16 14:19:43 INFO MemoryStore: MemoryStore cleared\n",
            "24/04/16 14:19:43 INFO BlockManager: BlockManager stopped\n",
            "24/04/16 14:19:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/04/16 14:19:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/04/16 14:19:43 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/04/16 14:19:43 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/04/16 14:19:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-01a62f4c-c64d-4304-9097-79a873d90d23/pyspark-a95f2990-df49-411e-925e-5a7ff4c6758e\n",
            "24/04/16 14:19:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-01a62f4c-c64d-4304-9097-79a873d90d23\n",
            "24/04/16 14:19:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8b560cf-884d-4299-806b-6dcf53ec45f3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobar salida"
      ],
      "metadata": {
        "id": "sf4whLva4qV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos que se ha creado la carpeta ./miSalida con el resultado"
      ],
      "metadata": {
        "id": "kriuXS9x4zo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./miSalida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo_uBc3F3UzC",
        "outputId": "77376248-aa1b-4a94-f838-5d72a01d2df4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "-rw-r--r-- 1 root root 133 Apr 16 14:19 part-00000\n",
            "-rw-r--r-- 1 root root 133 Apr 16 14:19 part-00001\n",
            "-rw-r--r-- 1 root root   0 Apr 16 14:19 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abrimos el archivo que contiene la salida"
      ],
      "metadata": {
        "id": "gA1iqmvh4yBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./miSalida/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_gWADNU4wl8",
        "outputId": "bf73d37f-6f53-4e38-9a13-6d4f23e66155"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Esto', 1)\n",
            "('prueba', 1)\n",
            "('para', 1)\n",
            "('contar', 1)\n",
            "('archivo', 1)\n",
            "('varias', 1)\n",
            "('programa', 1)\n",
            "('el', 1)\n",
            "('numero', 1)\n",
            "('cada', 1)\n",
            "('es', 1)\n",
            "('una', 1)\n",
            "('palabras', 1)\n",
            "('El', 2)\n",
            "('tiene', 1)\n",
            "('líneas', 1)\n",
            "('cuenta', 1)\n",
            "('de', 2)\n",
            "('apariciones', 1)\n",
            "('palabra', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecutar programa Spark Streaming"
      ],
      "metadata": {
        "id": "HpOgqtRCkDp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear programa"
      ],
      "metadata": {
        "id": "RWQgfFI1o6un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOF > mayusculasStreaming.py\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tsc = SparkContext(appName = \"miPrograma\")\n",
        "\tsc.setLogLevel(\"ERROR\")\n",
        "\tssc = StreamingContext(sc, 4)\n",
        "\n",
        "\tmisDatos = ssc.textFileStream(\"misDatosEnStreaming\")\n",
        "  #misDatos = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\tmisDatosMayusculas = misDatos.map(lambda line: line.upper())\n",
        "\n",
        "\tmisDatosMayusculas.pprint()\n",
        "\n",
        "\tssc.start()\n",
        "\tssc.awaitTermination()\n",
        "\n",
        "#Finaliza el programa\n",
        "EOF"
      ],
      "metadata": {
        "id": "BVkxuoZPl-JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANTE: no podemos leer desde un socket en Google Colab, entonces lo que hacemos es crear el programa leyendo de disco. Si el programa exige leer de un socket, entonce sólo hay que descomentar la línea y funcionará bien (aunque no funcionará en Colab)."
      ],
      "metadata": {
        "id": "bYe4pif6pjpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear datos"
      ],
      "metadata": {
        "id": "3UJC0DDro9dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANTE: los datos se empezarán a generar a los 15 segundos. Primero los datos que hay antes del primer \"sleep 5\" luego se esperará 5 segundos y se generarán los que hay a continuación hasta el siguiente \"sleep 5\". Y así sucesivamente.\n",
        "\n",
        "Nota: nada más ejecutar la celda de crear datos, hay que ejecutar el programa. Los primeros 15 segundos es para que de tiempo al programa a lanzarse"
      ],
      "metadata": {
        "id": "VuC7xOtjo-9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "\n",
        "sleep 15\n",
        "\n",
        "rm -r misDatosEnStreaming\n",
        "mkdir misDatosEnStreaming\n",
        "\n",
        "cat << EOF > misDatosEnStreaming/1\n",
        "Estos son los primeros datos que llegan en Streaming\n",
        "Esta es la segunda fila\n",
        "EOF\n",
        "\n",
        "sleep 5\n",
        "\n",
        "cat << EOF > misDatosEnStreaming/2\n",
        "Despues de 5 seg, llegan estos datos\n",
        "Mas datos\n",
        "EOF\n",
        "\n",
        "\n",
        "sleep 5\n",
        "\n",
        "cat << EOF > misDatosEnStreaming/3\n",
        "Terceros datos que llegan\n",
        "Ultima fila\n",
        "EOF"
      ],
      "metadata": {
        "id": "I43Tm2TIK8nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar programa"
      ],
      "metadata": {
        "id": "9s6dDWM2pcrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para finalizar el programa, pulsar en para la celda"
      ],
      "metadata": {
        "id": "sCrzAfNYpftv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! spark-submit mayusculasStreaming.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY3cOVO-mSZb",
        "outputId": "4fc6b702-9dba-4bad-ac8b-4938e22c284b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/02/29 16:33:56 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/02/29 16:33:56 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "24/02/29 16:33:56 INFO SparkContext: Java version 11.0.22\n",
            "24/02/29 16:33:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/02/29 16:33:57 INFO ResourceUtils: ==============================================================\n",
            "24/02/29 16:33:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/02/29 16:33:57 INFO ResourceUtils: ==============================================================\n",
            "24/02/29 16:33:57 INFO SparkContext: Submitted application: miPrograma\n",
            "24/02/29 16:33:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/02/29 16:33:57 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/02/29 16:33:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/02/29 16:33:57 INFO SecurityManager: Changing view acls to: root\n",
            "24/02/29 16:33:57 INFO SecurityManager: Changing modify acls to: root\n",
            "24/02/29 16:33:57 INFO SecurityManager: Changing view acls groups to: \n",
            "24/02/29 16:33:57 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/02/29 16:33:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/02/29 16:33:58 INFO Utils: Successfully started service 'sparkDriver' on port 36547.\n",
            "24/02/29 16:33:58 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/02/29 16:33:58 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/02/29 16:33:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/02/29 16:33:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/02/29 16:33:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/02/29 16:33:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-94f87488-9f29-433e-82ce-aa7149c2a306\n",
            "24/02/29 16:33:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/02/29 16:33:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/02/29 16:33:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/02/29 16:33:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/02/29 16:33:59 INFO Executor: Starting executor ID driver on host a63ea53c4471\n",
            "24/02/29 16:33:59 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "24/02/29 16:33:59 INFO Executor: Java version 11.0.22\n",
            "24/02/29 16:33:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/02/29 16:33:59 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7376c046 for default.\n",
            "24/02/29 16:33:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40495.\n",
            "24/02/29 16:33:59 INFO NettyBlockTransferService: Server created on a63ea53c4471:40495\n",
            "24/02/29 16:33:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/02/29 16:33:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a63ea53c4471, 40495, None)\n",
            "24/02/29 16:33:59 INFO BlockManagerMasterEndpoint: Registering block manager a63ea53c4471:40495 with 434.4 MiB RAM, BlockManagerId(driver, a63ea53c4471, 40495, None)\n",
            "24/02/29 16:33:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a63ea53c4471, 40495, None)\n",
            "24/02/29 16:33:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a63ea53c4471, 40495, None)\n",
            "/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
            "-------------------------------------------\n",
            "Time: 2024-02-29 16:34:04\n",
            "-------------------------------------------\n",
            "ESTOS SON LOS PRIMEROS DATOS QUE LLEGAN EN STREAMING\n",
            "ESTA ES LA SEGUNDA FILA\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-02-29 16:34:08\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-02-29 16:34:12\n",
            "-------------------------------------------\n",
            "DESPUES DE 5 SEG, LLEGAN ESTOS DATOS\n",
            "MAS DATOS\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-02-29 16:34:16\n",
            "-------------------------------------------\n",
            "TERCEROS DATOS QUE LLEGAN\n",
            "ULTIMA FILA\n",
            "\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=3>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o12.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/mayusculasStreaming.py\", line 16, in <module>\n",
            "    ssc.awaitTermination()\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/streaming/context.py\", line 239, in awaitTermination\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
            "py4j.protocol.Py4JError: An error occurred while calling o23.awaitTermination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecutar programa Structured Streaming"
      ],
      "metadata": {
        "id": "N8-HH89vqdWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear programa"
      ],
      "metadata": {
        "id": "9gZ7mwFvrZm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOF > mayusculas_structured_streaming.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "#Creamos una funcion que dado un registro lo convierta mayusculas\n",
        "@udf\n",
        "def mayusculas(record):\n",
        "    if record is not None:\n",
        "        return record.upper()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tspark = SparkSession.builder.appName(\"mayuscuclas_structured\").getOrCreate()\n",
        "\tspark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "\t#Leemos los datos\n",
        "\tlineas = spark.readStream.format(\"text\").load(\"misDatosEnStreaming\")\n",
        "\t'''\n",
        "\tlineas = spark \\\n",
        "\t\t    .readStream \\\n",
        "\t\t    .format(\"socket\") \\\n",
        "\t\t    .option(\"host\", \"localhost\") \\\n",
        "\t\t    .option(\"port\", 9999) \\\n",
        "\t\t    .load()\n",
        "\t'''\n",
        "\t#Cambiamos el nombre de la unica columna que tenemos\n",
        "\tlineas = lineas.withColumnRenamed(\"value\", \"colMinusculas\")\n",
        "\n",
        "\t#Creamos una nueva columna que convierte a mayusculas\n",
        "\tlineas_mayusculas = lineas.withColumn(\"colMaysuculas\", mayusculas(lineas.colMinusculas))\n",
        "\n",
        "\t#Creamos la query indicando que nos saque por consola el resultado\n",
        "\tquery = lineas_mayusculas \\\n",
        "\t    .writeStream \\\n",
        "\t    .format(\"console\") \\\n",
        "\t    .start()\n",
        "\n",
        "\t#Ejecutamos la query\n",
        "\tquery.awaitTermination()\n",
        "EOF"
      ],
      "metadata": {
        "id": "B0gAM6xYnN98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANTE: no podemos leer desde un socket en Google Colab, entonces lo que hacemos es crear el programa leyendo de disco. Si el programa exige leer de un socket, entonce sólo hay que descomentar la línea y funcionará bien (aunque no funcionará en Colab)."
      ],
      "metadata": {
        "id": "r2318aJTrK60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear datos"
      ],
      "metadata": {
        "id": "B2xhNfcTrK62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANTE: los datos se empezarán a generar a los 15 segundos. Primero los datos que hay antes del primer \"sleep 5\" luego se esperará 5 segundos y se generarán los que hay a continuación hasta el siguiente \"sleep 5\". Y así sucesivamente.\n",
        "\n",
        "Nota: nada más ejecutar la celda de crear datos, hay que ejecutar el programa. Los primeros 15 segundos es para que de tiempo al programa a lanzarse"
      ],
      "metadata": {
        "id": "1mzMnPDhrK63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "\n",
        "sleep 15\n",
        "\n",
        "rm -r misDatosEnStreaming\n",
        "mkdir misDatosEnStreaming\n",
        "\n",
        "\n",
        "cat << EOF > misDatosEnStreaming/1\n",
        "Estos son los primeros datos que llegan en Streaming\n",
        "Esta es la segunda fila\n",
        "EOF\n",
        "\n",
        "sleep 5\n",
        "\n",
        "cat << EOF > misDatosEnStreaming/2\n",
        "Despues de 5 seg, llegan estos datos\n",
        "Mas datos\n",
        "EOF\n",
        "\n",
        "\n",
        "sleep 5\n",
        "\n",
        "cat << EOF > misDatosEnStreaming/3\n",
        "Terceros datos que llegan\n",
        "Ultima fila\n",
        "EOF"
      ],
      "metadata": {
        "id": "LmuPOIcMrK64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar programa"
      ],
      "metadata": {
        "id": "rYCD4XVIrK65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para finalizar el programa, pulsar en para la celda"
      ],
      "metadata": {
        "id": "o149p-b6rK66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! spark-submit mayusculas_structured_streaming.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2NgMNvOrMr1",
        "outputId": "41c09abe-a12b-43ab-c291-589ed319c951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/02/29 16:32:55 INFO SparkContext: Running Spark version 3.5.1\n",
            "24/02/29 16:32:55 INFO SparkContext: OS info Linux, 6.1.58+, amd64\n",
            "24/02/29 16:32:55 INFO SparkContext: Java version 11.0.22\n",
            "24/02/29 16:32:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/02/29 16:32:55 INFO ResourceUtils: ==============================================================\n",
            "24/02/29 16:32:55 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/02/29 16:32:55 INFO ResourceUtils: ==============================================================\n",
            "24/02/29 16:32:55 INFO SparkContext: Submitted application: mayuscuclas_structured\n",
            "24/02/29 16:32:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/02/29 16:32:55 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/02/29 16:32:55 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/02/29 16:32:55 INFO SecurityManager: Changing view acls to: root\n",
            "24/02/29 16:32:55 INFO SecurityManager: Changing modify acls to: root\n",
            "24/02/29 16:32:55 INFO SecurityManager: Changing view acls groups to: \n",
            "24/02/29 16:32:55 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/02/29 16:32:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/02/29 16:32:56 INFO Utils: Successfully started service 'sparkDriver' on port 40313.\n",
            "24/02/29 16:32:56 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/02/29 16:32:56 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/02/29 16:32:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/02/29 16:32:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/02/29 16:32:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/02/29 16:32:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-53b32b8e-2adf-4080-b52a-860e3bcd7393\n",
            "24/02/29 16:32:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/02/29 16:32:56 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/02/29 16:32:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/02/29 16:32:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/02/29 16:32:57 INFO Executor: Starting executor ID driver on host a63ea53c4471\n",
            "24/02/29 16:32:57 INFO Executor: OS info Linux, 6.1.58+, amd64\n",
            "24/02/29 16:32:57 INFO Executor: Java version 11.0.22\n",
            "24/02/29 16:32:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/02/29 16:32:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4b086ba5 for default.\n",
            "24/02/29 16:32:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34325.\n",
            "24/02/29 16:32:57 INFO NettyBlockTransferService: Server created on a63ea53c4471:34325\n",
            "24/02/29 16:32:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/02/29 16:32:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a63ea53c4471, 34325, None)\n",
            "24/02/29 16:32:57 INFO BlockManagerMasterEndpoint: Registering block manager a63ea53c4471:34325 with 434.4 MiB RAM, BlockManagerId(driver, a63ea53c4471, 34325, None)\n",
            "24/02/29 16:32:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a63ea53c4471, 34325, None)\n",
            "24/02/29 16:32:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a63ea53c4471, 34325, None)\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|       colMinusculas|       colMaysuculas|\n",
            "+--------------------+--------------------+\n",
            "|Estos son los pri...|ESTOS SON LOS PRI...|\n",
            "|Esta es la segund...|ESTA ES LA SEGUND...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|       colMinusculas|       colMaysuculas|\n",
            "+--------------------+--------------------+\n",
            "|Despues de 5 seg,...|DESPUES DE 5 SEG,...|\n",
            "|           Mas datos|           MAS DATOS|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|       colMinusculas|       colMaysuculas|\n",
            "+--------------------+--------------------+\n",
            "|Terceros datos qu...|TERCEROS DATOS QU...|\n",
            "|         Ultima fila|         ULTIMA FILA|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=3>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o12.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/mayusculas_structured_streaming.py\", line 29, in <module>\n",
            "    query.awaitTermination()\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/streaming/query.py\", line 221, in awaitTermination\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "  File \"/usr/local/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
            "py4j.protocol.Py4JError: An error occurred while calling o47.awaitTermination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tssil5OEsvvu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/288756/VisArtificial/blob/master/P5_Segmentaci%C3%B3n_sem%C3%A1ntica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eWxNbawpisr"
      },
      "source": [
        "## **Práctica 5: Segmentación semántica**\n",
        "\n",
        "La **segmentación semántica** consiste identificar la ubicación de un objeto en una imagen y marcarlo con una caja delimitadora o *bounding box*. Es habitual que, además de localizar el objeto, se clasifique en una de las clases objetivo (clasificación y localización). Por su parte, el proceso de localización implica determinar las coordenadas del *bounding box*, por lo que se puede abordar como un problema de regresión.\n",
        "\n",
        "A continuación, vamos a ver un ejemplo de segmentación semántica utilizando la librería [TensorFlow](https://www.tensorflow.org/). Para ello utilizaremos una red pre-entrenada en ImageNet como extractor de características y resolveremos un problema de clasificación a nivel de píxel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz_7t-Jpp49R"
      },
      "source": [
        "Antes de empezar, vamos a utilizar el método [set_random_seed()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed) para establecer el valor de la **semilla** y garantizar la reproducibilidad de los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFz3XfKxpcpH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "\n",
        "seed = 121\n",
        "set_random_seed(seed)  # establece todas las semillas aleatorias del programa (Python, NumPy y TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojS1mHIBp7Ku"
      },
      "source": [
        "### **1. Conjunto de datos**\n",
        "\n",
        "En esta práctica vamos a utilizar un conjunto para segmentación semántica denominado **UNIMIB**, adaptado de la versión original [UNIMIB 2016 Food Database](https://zenodo.org/records/4126613). Para ello, es necesario subir a Google Drive el fichero [UNIMIB2016_SS.zip](https://drive.google.com/file/d/1Rf34GIIcfSVM1_6pHQuxmUz_Vgy_0tCv/view?usp=sharing) que contiene:\n",
        "\n",
        "*   Una carpeta `images` con dos directorios que contienen las imágenes originales:\n",
        "> *  `train`, con 650 imágenes de entrenamiento.\n",
        "> *  `val`, con 360 imágenes de validación.\n",
        "\n",
        "*   Una carpeta `masks` con los mismos directorios, uno para cada partición de los datos, en los que hay una máscara de segmentación para cada imagen original.\n",
        "\n",
        "El conjunto incluye regiones de 66 clases diferentes, 65 de las cuales corresponden a platos de comida y 1 al fondo de la imagen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbzO1zvUvIxu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montar el Google Drive en el directorio del proyecto y descomprimir el fichero con los datos\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -n '/content/gdrive/My Drive/datasets/UNIMIB2016_SS.zip' >> /dev/null  # ACTUALIZAR: ruta al fichero comprimido"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez hemos descomprimido el fichero, vamos a especificar la información dependiente del conjunto de datos y a crear cuatro variables que contengan la ruta a los ficheros con las imágenes originales y las máscaras, para las particiones de entrenamiento y validación."
      ],
      "metadata": {
        "id": "7LzkRKTPztTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6g-XWjfwB4z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Especificar información dependiente del conjunto de datos\n",
        "images_path = 'UNIMIB2016_SS/images/'\n",
        "masks_path = 'UNIMIB2016_SS/masks/'\n",
        "training_path = \"train/\"\n",
        "validation_path = \"val/\"\n",
        "\n",
        "img_width = img_height = 224  # dimensiones de la imagen\n",
        "n_channels = 3                # número de canales\n",
        "n_classes =66                 # número de clases\n",
        "\n",
        "x_train = sorted(glob(os.path.join(images_path + training_path, \"*.png\")))\n",
        "y_train = sorted(glob(os.path.join(masks_path + training_path, \"*.png\")))\n",
        "n_samples_train = len(x_train)\n",
        "\n",
        "x_val = sorted(glob(os.path.join(images_path + validation_path, \"*.png\")))\n",
        "y_val = sorted(glob(os.path.join(masks_path + validation_path, \"*.png\")))\n",
        "n_samples_val = len(x_val)\n",
        "\n",
        "print(\"Número de ejemplos del conjunto de entrenamiento: \", n_samples_train)\n",
        "print(\"Número de ejemplos del conjunto de validación: \", n_samples_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c0qJIK41KZB"
      },
      "source": [
        "#### **1.1. Procesamiento de imágenes**\n",
        "\n",
        "El siguiente paso consiste en procesar las imágenes y las máscaras. Para ello definiremos, en primer lugar, dos funciones que leen y pre-procesan, respectivamente, las imágenes y las máscaras:\n",
        "\n",
        "*  Redimensionar y normalizar las imágenes.\n",
        "*  Redimensionar las máscaras con un tipo de interpolación adecuado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzIyvW7gwV9n"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (img_width, img_height))\n",
        "    x = x/255.0  # normalización\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x, (img_width, img_height), interpolation = cv2.INTER_NEAREST)  # interpolación sin solapamiento\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, vamos a definir dos funciones para generar los conjuntos de entrenamiento y validación utilizando la clase [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
      ],
      "metadata": {
        "id": "e7RtTA7Q1_wd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZjrMBrU2E0J"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def tf_parse(x, y):\n",
        "\n",
        "    # Función interna de apoyo\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.uint8])\n",
        "    x.set_shape([img_width, img_height, 3])  # imágenes RGB\n",
        "    y.set_shape([img_width, img_height, 1])  # máscaras (id labels)\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset_training(x, y, batch):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(n_samples_train)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(1)\n",
        "    return dataset\n",
        "\n",
        "def tf_dataset_validation(x, y, batch):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(1)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, crearemos los conjuntos de datos y prepararemos los lotes haciendo uso de las funciones anteriores."
      ],
      "metadata": {
        "id": "IlBTeMsT2vTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rRQRFbD48jF"
      },
      "outputs": [],
      "source": [
        "# Crear los conjuntos de datos y preparar los lotes\n",
        "batch_size = 2\n",
        "train_dataset = tf_dataset_training(x_train, y_train, batch_size)\n",
        "val_dataset = tf_dataset_validation(x_val, y_val, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2. Visualización de imágenes**\n",
        "\n",
        "En esta sección visualizaremos algunas imágenes del conjunto de entrenamiento junto con sus máscaras de segmentación.\n",
        "\n",
        "Para ello definiremos, en primer lugar, una función que nos permita leer las imágenes para mostrarlas correctamente con la librería [Matplotlib](https://matplotlib.org/)."
      ],
      "metadata": {
        "id": "mRyICAjb3jiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LN8qWDq5KLD"
      },
      "outputs": [],
      "source": [
        "def read_and_rgb(x):\n",
        "  x = cv2.imread(x)\n",
        "  x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, representaremos las cuatro primeras imágenes del conjunto de entrenamiento con sus respectivas máscaras de segmentación, utilizando un formato 8x8."
      ],
      "metadata": {
        "id": "yAr1y8ET4Aks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaznN5Ij5Plt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imágenes originales\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "a = fig.add_subplot(1, 4, 1)\n",
        "imgplot = plt.imshow(read_and_rgb(x_train[0]))\n",
        "\n",
        "a = fig.add_subplot(1, 4, 2)\n",
        "imgplot = plt.imshow(read_and_rgb(x_train[1]))\n",
        "imgplot.set_clim(0.0, 0.7)\n",
        "\n",
        "a = fig.add_subplot(1, 4, 3)\n",
        "imgplot = plt.imshow(read_and_rgb(x_train[2]))\n",
        "imgplot.set_clim(0.0, 1.4)\n",
        "\n",
        "a = fig.add_subplot(1, 4, 4)\n",
        "imgplot = plt.imshow(read_and_rgb(x_train[3]))\n",
        "imgplot.set_clim(0.0, 2.1)\n",
        "\n",
        "# Máscaras de segmentación\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "a = fig.add_subplot(1, 4, 1)\n",
        "imgplot = plt.imshow(read_and_rgb(y_train[0]))\n",
        "\n",
        "a = fig.add_subplot(1, 4, 2)\n",
        "imgplot = plt.imshow(read_and_rgb(y_train[1]))\n",
        "imgplot.set_clim(0.0, 0.7)\n",
        "\n",
        "a = fig.add_subplot(1, 4, 3)\n",
        "imgplot = plt.imshow(read_and_rgb(y_train[2]))\n",
        "imgplot.set_clim(0.0, 1.4)\n",
        "\n",
        "a = fig.add_subplot(1, 4, 4)\n",
        "imgplot = plt.imshow(read_and_rgb(y_train[3]))\n",
        "imgplot.set_clim(0.0, 1.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pseZThZ_5QGl"
      },
      "source": [
        "### **2. Red convolucional**\n",
        "\n",
        "La CNN que vamos a definir está compuesta por la base convolucional de una [MobileNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2), pre-entrenada con [ImageNet](https://www.image-net.org/). Esta parte del modelo constituye el denominado *downsampling path* y sus parámetros permanecerán fijos, por lo que utilizaremos la MobileNetV2 pre-entrenada como extractor de características.\n",
        "\n",
        "Con respecto al *upsampling path*, definiremos una estructura basada convoluciones transpuestas y en un bloque convolucional que definiremos previamente.\n",
        "\n",
        "*   [Capa convolución transpuesta](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose): `Conv2DTranspose(n_filters, kernel_size)` crea una capa con `n_filters` de tamaño `kernel_size` que se aplican a los datos de entrada para producir un tensor de salidas. Si `use_bias` es `True`, se crea un vector de sesgo y se suma a las salidas. Si `activation` no es `None`, también se aplica la función de activación especificada a las salidas. Otros parámetros relevantes:\n",
        "> * `strides`: un entero o tupla/lista de un entero, especificando el paso de la convolución transpuesta. `strides>1` es incompatible con `dilation_rate>1`.\n",
        "> * `padding`: `valid`, que significa sin relleno; o `same`, que da como resultado un relleno de ceros uniforme (izquieda/derecha y arriba/abajo). Si `padding='same'`y `strides=1`, la salida tiene el mismo tamaño que la entrada.\n",
        "> * `activation`: función de activación (`relu`, `sigmoid`, etc.) Por defecto, `activation=None` (es decir, no se utiliza función de activación).\n",
        "*   **Bloque convolucional:** Definidio ad-hoc para esta arquitectura, está compuesto de una capa convolucional con la función de activación `ReLU`. Permite aplicar una capa de `BatchNormalization` antes de la función de activación, en función de un valor *Booleano* que recibe como parámetro, por lo que este último paso se aplica en una capa independiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uChIiz4N54t8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, Activation, Dropout, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
        "    x = Conv2D(filters = n_filters, kernel_size = kernel_size, kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def get_model(n_filters = 16, batchnorm = True, dropout = 0.1):\n",
        "\n",
        "    inputs = Input(shape=(img_width, img_height, n_channels), name=\"input_image\")\n",
        "\n",
        "    # ENCODER: MobileNetV2\n",
        "    encoder = MobileNetV2(input_tensor=inputs, include_top=False)\n",
        "\n",
        "    BASE_WEIGHT_PATH = ('https://github.com/fchollet/deep-learning-models/releases/download/v0.6/')\n",
        "    model_name = 'mobilenet_%s_%d_tf_no_top.h5' % ('1_0', 224)\n",
        "    weight_path = BASE_WEIGHT_PATH + model_name\n",
        "    weights_path = tf.keras.utils.get_file(model_name, weight_path)\n",
        "    encoder.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "\n",
        "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
        "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
        "    x = encoder_output\n",
        "\n",
        "    x_skip_1 = encoder.get_layer(skip_connection_names[-1]).output  # 224x224\n",
        "    x_skip_2 = encoder.get_layer(skip_connection_names[-2]).output  # 112x112\n",
        "    x_skip_3 = encoder.get_layer(skip_connection_names[-3]).output  # 56x56\n",
        "    x_skip_4 = encoder.get_layer(skip_connection_names[-4]).output  # 28x28\n",
        "\n",
        "    # DECODER\n",
        "    u6 = Conv2DTranspose(n_filters * 13, (3, 3), strides = (2, 2), padding = 'same')(x)\n",
        "    u6 = concatenate([u6, x_skip_1])\n",
        "    c6 = conv2d_block(u6, n_filters * 13, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p6 = Dropout(dropout)(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(n_filters * 12, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
        "    u7 = concatenate([u7, x_skip_2])\n",
        "    c7 = conv2d_block(u7, n_filters * 12, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p7 = Dropout(dropout)(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters * 11, (3, 3), strides = (2, 2), padding = 'same')(p7)\n",
        "    u8 = concatenate([u8, x_skip_3])\n",
        "    c8 = conv2d_block(u8, n_filters * 11, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p7 = Dropout(dropout)(c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters * 10, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
        "    u9 = concatenate([u9, x_skip_4])\n",
        "    c9 = conv2d_block(u9, n_filters * 10, kernel_size = 3, batchnorm = batchnorm)\n",
        "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZKJNHIA7NZZ"
      },
      "source": [
        "### **3. Entrenamiento**\n",
        "\n",
        "Una vez definida la arquitectura del modelo de segmentación semántica, el siguiente paso consiste en configurarlo para el entrenamiento. Para ello definiremos, en primer lugar, el coeficiente de similitud Dice para utilizarlo como métrica de evaluación y para definir la función de pérdida.\n",
        "\n",
        "*  **Coeficiente de similitud Dice:** Mide el solapamiento entre dos regiones, la real (máscara) y la que predice el modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1e-7):\n",
        "    y_true_f = K.flatten(K.one_hot(K.cast(y_true, 'int32'), num_classes=n_classes)[Ellipsis,1:])\n",
        "    y_pred_f = K.flatten(y_pred[...,1:])\n",
        "    intersect = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    denom = K.sum(y_true_f + y_pred_f, axis=-1)\n",
        "    return K.mean((2. * intersect / (denom + smooth)))\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)"
      ],
      "metadata": {
        "id": "EENY98haNPCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, compilaremos el modelo utilizando el método [`compile()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile), siendo estos algunos de sus parámetros más relevantes:\n",
        "\n",
        "* `loss`: función de pérdida (`mean_squared_error`, `binary_crossentropy`, `categorical_crossentropy`, etc.). En la web de `TensorFlow` puedes encontrar otras [funciones de pérdida](https://www.tensorflow.org/api_docs/python/tf/keras/losses).\n",
        "* `metrics`: métricas que se evalúan para los datos de entrenamiento y validación (`accuracy`, etc.). En la web de `TensorFlow` puedes encontrar otras [métricas](https://www.tensorflow.org/api_docs/python/tf/keras/metrics).\n",
        "* `optimizer`: nombre del optimizador (`Adam`, `RMSProp`, etc.) y tasa de aprendizaje (`learning_rate`). En la web de `TensorFlow` puedes encontrar otros [optimizadores](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
        "* `callbacks`: callabacks que se utilizarán durante el entrenamiento (`ReduceLROnPlateau`, `EarlyStopping`, etc). En la web de `TensorFlow` puedes encontrar otros [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)."
      ],
      "metadata": {
        "id": "nx1JrbluNTVg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjc5XZlM6wTl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Crear el modelo\n",
        "model = get_model(n_filters = 16, batchnorm = True, dropout = 0.1)\n",
        "\n",
        "# Congelar los parámetros de las capas del codificador\n",
        "encoder_layers = model.layers[0:-22]\n",
        "for layer in encoder_layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compilar el modelo con el optimizador Adam\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(loss=dice_loss, optimizer=opt, metrics=[dice_coef, 'accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, vamos a entrenar el modelo para buscar los parámetros que hagan mínima la función de pérdida. Para ello utilizaremos el método [`fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), que necesita que le suministremos los datos de entrenamiento y validación, y el número de *epochs*.\n",
        "\n",
        "En este caso, además, hemos configurado dos callbacks que se utilizarán durante el entrenamiento:\n",
        "\n",
        "*  [EarlyStopping](tf.keras.callbacks.EarlyStopping): Detiene el entrenamiento cuando una métrica supervisada ha dejado de mejorar.\n",
        "*  [ReduceLROnPlateau](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau): Reduce la tasa de aprendizaje cuando una métrica ha dejado de mejorar."
      ],
      "metadata": {
        "id": "vHIjJqt5O7bX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZZ7xU2S7mPd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_dice_coef', patience=20, mode='max')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=np.ceil(len(x_train)/batch_size),\n",
        "    validation_steps=np.ceil(len(x_val)/batch_size),\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtUaXxkn7yue"
      },
      "source": [
        "### **4. Predicción**\n",
        "\n",
        "Una vez entrenado el modelo, es posible utilizarlo para segmentar semánticamente una imagen.\n",
        "\n",
        "Para ello, vamos a definir dos funciones que permiten visualizar la máscara segmentada de una imagen original y la predicción del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1-0ileG7w9u"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask):\n",
        "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n",
        "    return pred_mask\n",
        "\n",
        "def display_sample(display_list):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    title = ['True mask', 'Predicted mask']\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        plt.imshow(display_list[i])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, utilizaremos una imagen del conjunto de validación para mostrar un ejemplo de predicción con el modelo previamente entrenado."
      ],
      "metadata": {
        "id": "zdQ805zTsyRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziE3Al7F7-YC"
      },
      "outputs": [],
      "source": [
        "for image, mask in val_dataset.take(3):\n",
        "    sample_image, sample_mask = image, mask\n",
        "\n",
        "img = sample_image[0][tf.newaxis, ...]\n",
        "prediction = model.predict(img)\n",
        "pred_mask = create_mask(prediction)\n",
        "display_sample([sample_mask[0], pred_mask[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Ejercicios**\n",
        "\n",
        "A continuación, se propone un ejercicio para su resolución."
      ],
      "metadata": {
        "id": "UkQT-oDRV39t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EJERCICIO 1**\n",
        "\n",
        "Dado el modelo original, ¿qué cambios tendríamos que hacer para sustituir la base convolucional de la MobileNetV2 por una VGG16?"
      ],
      "metadata": {
        "id": "i4VbUoV7V8FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do: Solución al ejercicio 1"
      ],
      "metadata": {
        "id": "SzzM9nJDV99N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}